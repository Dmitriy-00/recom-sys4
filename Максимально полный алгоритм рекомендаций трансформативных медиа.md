# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö –º–µ–¥–∏–∞

> **–¶–µ–ª—å:** –°–æ–∑–¥–∞—Ç—å state-of-the-art —Å–∏—Å—Ç–µ–º—É —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π, –∏—Å–ø–æ–ª—å–∑—É—é—â—É—é –í–°–ï –¥–æ—Å—Ç—É–ø–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã

---

## üìä –¢–ê–ö–°–û–ù–û–ú–ò–Ø –í–°–ï–• –ö–ê–¢–ï–ì–û–†–ò–ô –î–ê–ù–ù–´–•

### 1. –ú–ê–¢–ï–†–ò–ê–õ–´ (89+ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)

#### 1.1 –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
```python
material_metadata = {
    'identification': {
        'title': str,
        'original_title': str,
        'year': int,
        'creator': str,  # —Ä–µ–∂–∏—Å—Å—ë—Ä/–∞–≤—Ç–æ—Ä
        'country': str,
        'language': str,
        'type': ['film', 'book', 'game', 'series']
    },
    
    'physical': {
        'runtime': int,  # –¥–ª—è —Ñ–∏–ª—å–º–æ–≤ (–º–∏–Ω—É—Ç—ã)
        'pages': int,    # –¥–ª—è –∫–Ω–∏–≥
        'episodes': int, # –¥–ª—è —Å–µ—Ä–∏–∞–ª–æ–≤
        'asl': float,    # Average Shot Length (—Å–µ–∫—É–Ω–¥—ã)
    }
}
```

#### 1.2 –ñ–∞–Ω—Ä—ã –∏ —Ç–∞–∫—Å–æ–Ω–æ–º–∏—è
```python
genre_taxonomy = {
    'primary_genre': str,  # –æ—Å–Ω–æ–≤–Ω–æ–π –∂–∞–Ω—Ä
    'subgenres': List[str],
    'genre_purity': float,  # 0-1, –Ω–∞—Å–∫–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π –∂–∞–Ω—Ä
    'genre_blend': Dict[str, float],  # {"scifi": 0.7, "thriller": 0.3}
    
    # –ú–µ—Ç–∞-–∂–∞–Ω—Ä—ã
    'narrative_mode': [
        'realistic', 'fantastic', 'surreal', 'abstract',
        'documentary', 'experimental', 'hybrid'
    ],
    
    'tone': [
        'serious', 'comedic', 'satirical', 'tragic',
        'whimsical', 'dark', 'hopeful', 'nihilistic'
    ]
}
```

#### 1.3 –í–∏–∑—É–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (–¥–ª—è —Ñ–∏–ª—å–º–æ–≤)
```python
visual_characteristics = {
    'cinematography': {
        'asl': float,  # Average Shot Length
        'asl_variance': float,  # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –¥–ª–∏–Ω—ã –∫–∞–¥—Ä–æ–≤
        'camera_movement': {
            'static_ratio': float,  # % —Å—Ç–∞—Ç–∏—á–Ω—ã—Ö –∫–∞–¥—Ä–æ–≤
            'handheld_ratio': float,
            'tracking_shots': float,
            'crane_shots': float
        },
        'shot_composition': {
            'symmetry': float,  # 1-10
            'depth_of_field': ['shallow', 'deep', 'variable'],
            'rule_of_thirds_adherence': float,  # 0-1
        }
    },
    
    'color': {
        'palette_dominant': ['warm', 'cool', 'neutral', 'monochrome'],
        'saturation': float,  # 1-10
        'contrast': float,    # 1-10
        'color_grading_style': str,  # "naturalistic", "stylized", etc
        'symbolic_colors': List[Dict],  # [{color: 'red', meaning: 'passion', frequency: 0.3}]
        'color_temperature': float,  # Kelvin
        'color_harmony': ['monochromatic', 'complementary', 'triadic', 'analogous']
    },
    
    'lighting': {
        'style': ['high_key', 'low_key', 'naturalistic', 'expressionistic'],
        'contrast_ratio': float,
        'shadow_usage': float,  # 1-10
        'practical_vs_artificial': float  # 0=–≤—Å–µ –ø—Ä–∞–∫—Ç–∏—á–Ω–æ–µ, 1=–≤—Å–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–µ
    },
    
    'visual_density': {
        'information_per_frame': float,  # 1-10
        'complexity': float,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–∏–∑—É–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤
        'focus_points': float,  # —Å—Ä–µ–¥–Ω–µ–µ —á–∏—Å–ª–æ —Ç–æ—á–µ–∫ —Ñ–æ–∫—É—Å–∞
    },
    
    'visual_style': {
        'realism_level': float,  # 1-10
        'stylization': float,    # 1-10
        'references': List[str],  # –≤–∏–∑—É–∞–ª—å–Ω—ã–µ —Ä–µ—Ñ–µ—Ä–µ–Ω—Å—ã
        'period_accuracy': float  # –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö
    }
}
```

#### 1.4 –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è –∫–Ω–∏–≥)
```python
text_characteristics = {
    'readability': {
        'flesch_reading_ease': float,  # 0-100
        'flesch_kincaid_grade': float,
        'gunning_fog_index': float,
        'smog_index': float,
        'automated_readability_index': float,
        'coleman_liau_index': float,
        'estimated_reading_level': str  # 'elementary', 'high_school', etc
    },
    
    'sentence_structure': {
        'avg_sentence_length': float,  # —Å–ª–æ–≤
        'sentence_length_variance': float,
        'simple_sentences_ratio': float,
        'compound_sentences_ratio': float,
        'complex_sentences_ratio': float,
        'avg_clause_depth': float  # –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
    },
    
    'vocabulary': {
        'lexical_diversity': float,  # TTR (type-token ratio)
        'hapax_legomena_ratio': float,  # —Å–ª–æ–≤–∞ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è —Ä–∞–∑
        'rare_words_ratio': float,  # –≤–Ω–µ top 5000
        'technical_terminology_density': float,
        'foreign_words_ratio': float,
        'neologisms_count': int,
        'archaic_words_ratio': float
    },
    
    'style': {
        'formality': float,  # 1-10
        'abstractness': float,  # concrete vs abstract language
        'imagery_density': float,  # –æ–±—Ä–∞–∑–æ–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É
        'metaphor_density': float,  # –º–µ—Ç–∞—Ñ–æ—Ä –Ω–∞ 1000 —Å–ª–æ–≤
        'simile_density': float,
        'personification_density': float,
        'sensory_language_ratio': Dict[str, float]  # visual, auditory, etc
    },
    
    'prose_rhythm': {
        'avg_syllables_per_word': float,
        'polysyllabic_words_ratio': float,
        'rhythm_pattern': str,  # 'flowing', 'staccato', 'varied'
        'alliteration_frequency': float,
        'assonance_frequency': float
    },
    
    'dialogue': {
        'dialogue_to_narration_ratio': float,
        'dialogue_realism': float,  # 1-10
        'dialect_usage': bool,
        'subtext_density': float  # —Å–∫—Ä—ã—Ç—ã–µ —Å–º—ã—Å–ª—ã
    }
}
```

#### 1.5 –ê—É–¥–∏–æ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
```python
audio_characteristics = {
    'soundtrack': {
        'type': ['orchestral', 'ambient', 'electronic', 'minimal', 'none'],
        'composer': str,
        'music_density': float,  # % –≤—Ä–µ–º–µ–Ω–∏ —Å –º—É–∑—ã–∫–æ–π
        'leitmotif_usage': bool,
        'diegetic_vs_nondiegetic': float,  # 0=–≤—Å–µ –¥–∏–µ–≥–µ—Ç–∏—á–µ—Å–∫–æ–µ
        'emotional_range': float  # –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –º—É–∑—ã–∫–∏
    },
    
    'sound_design': {
        'complexity': float,  # 1-10
        'naturalism': float,  # 1-10 (—Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å)
        'sound_layers': int,  # —Å—Ä–µ–¥–Ω–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–æ—ë–≤
        'silence_usage': float,  # % —Ç–∏—à–∏–Ω—ã
        'ambient_sound_density': float
    },
    
    'dialogue': {
        'dialogue_density': float,  # 1-10
        'dialogue_clarity': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑–±–æ—Ä—á–∏–≤–æ
        'overlapping_dialogue': bool,
        'silence_vs_dialogue_ratio': float,
        'voice_over_usage': bool
    },
    
    'audio_mood': {
        'tension': float,  # 1-10
        'warmth': float,
        'energy': float,
        'melancholy': float
    }
}
```

#### 1.6 –¢–µ–º–ø–æ—Ä–∏—Ç–º –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
```python
pacing_structure = {
    'tempo': {
        'overall': float,  # 1-10 (slow to fast)
        'variation': float,  # 1-10 (constant to varied)
        'acceleration_pattern': str,  # 'constant', 'building', 'declining', 'wave'
    },
    
    'act_structure': {
        'type': ['3-act', '5-act', '7-point', 'kishotenketsu', 'non-linear', 'episodic'],
        'adherence': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–≥–æ —Å–ª–µ–¥—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        'plot_points': {
            'inciting_incident': float,  # % –æ—Ç –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            'first_plot_point': float,
            'midpoint': float,
            'second_plot_point': float,
            'climax': float,
            'resolution': float
        }
    },
    
    'narrative_structure': {
        'linearity': float,  # 0=–Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ, 1=–ª–∏–Ω–µ–π–Ω–æ–µ
        'timeline_complexity': int,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ª–∏–Ω–∏–π
        'flashback_ratio': float,
        'flash_forward_ratio': float,
        'parallel_narratives': int,
        'nested_stories': int,  # story within story
        'circular_structure': bool
    },
    
    'chapter_structure': {  # –¥–ª—è –∫–Ω–∏–≥
        'avg_chapter_length': float,  # —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        'chapter_length_variance': float,
        'cliffhanger_frequency': float
    }
}
```

#### 1.7 –ü–µ—Ä—Å–æ–Ω–∞–∂–∏
```python
character_parameters = {
    'protagonist': {
        'archetype': str,  # Hero, Anti-hero, Everyman, etc
        'transformation_magnitude': float,  # 1-10
        'transformation_direction': ['positive', 'negative', 'ambiguous', 'cyclical', 'none'],
        'agency': float,  # 1-10, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∞–∫—Ç–∏–≤–µ–Ω
        'moral_alignment': {  # D&D style
            'good_evil_axis': float,  # -1 (evil) to 1 (good)
            'lawful_chaotic_axis': float  # -1 (chaotic) to 1 (lawful)
        },
        'relatability': float,  # 1-10
        'competence': float  # 1-10
    },
    
    'character_count': {
        'total_named': int,
        'major_characters': int,  # > 10% screen/page time
        'pov_characters': int,
        'character_density': float  # –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –Ω–∞ —á–∞—Å/100 —Å—Ç—Ä–∞–Ω–∏—Ü
    },
    
    'character_complexity': {
        'psychological_depth': float,  # 1-10
        'moral_ambiguity': float,  # 0-1
        'internal_conflict': float,  # 1-10
        'contradictions': float,  # –µ—Å—Ç—å –ª–∏ –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–µ
        'growth_potential': float  # —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –∏–∑–º–µ–Ω–µ–Ω–∏—é
    },
    
    'archetypes_present': List[str],  # Jung/Campbell/Vogler
    
    'character_relationships': {
        'network_density': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–≤—è–∑–∞–Ω—ã –ø–µ—Ä—Å–æ–Ω–∞–∂–∏
        'relationship_complexity': float,  # 1-10
        'conflict_types': List[str],  # internal, interpersonal, vs society, etc
        'power_dynamics': str  # 'hierarchical', 'equal', 'fluid'
    },
    
    'representation': {
        'gender_distribution': Dict[str, float],
        'age_distribution': Dict[str, float],
        'diversity_score': float,  # —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –ø–æ –≤—Å–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        'stereotype_reliance': float  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ª–∞–≥–∞–µ—Ç—Å—è –Ω–∞ —Å—Ç–µ—Ä–µ–æ—Ç–∏–ø—ã
    }
}
```

#### 1.8 –¢–µ–º–∞—Ç–∏–∫–∞ –∏ –∏–¥–µ–∏
```python
thematic_parameters = {
    'themes': {
        'primary_themes': List[Dict],  # [{'theme': str, 'prominence': float, 'treatment': str}]
        'subthemes': List[str],
        'thematic_density': float,  # —Ç–µ–º –Ω–∞ –µ–¥–∏–Ω–∏—Ü—É –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        'theme_integration': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ä–≥–∞–Ω–∏—á–Ω–æ –≤–ø–ª–µ—Ç–µ–Ω—ã
        'universal_vs_specific': float  # —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ vs —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ —Ç–µ–º—ã
    },
    
    'philosophical_depth': {
        'overall': float,  # 1-10
        'epistemological': float,  # –≤–æ–ø—Ä–æ—Å—ã –ø–æ–∑–Ω–∞–Ω–∏—è
        'metaphysical': float,  # –≤–æ–ø—Ä–æ—Å—ã –±—ã—Ç–∏—è
        'ethical': float,  # –º–æ—Ä–∞–ª—å–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã
        'existential': float,  # –≤–æ–ø—Ä–æ—Å—ã —Å–º—ã—Å–ª–∞
        'political': float,  # –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–¥–µ–∏
        'aesthetic': float  # –≤–æ–ø—Ä–æ—Å—ã –∫—Ä–∞—Å–æ—Ç—ã/–∏—Å–∫—É—Å—Å—Ç–≤–∞
    },
    
    'intellectual_complexity': {
        'abstraction_level': float,  # 1-10
        'prerequisite_knowledge': List[str],  # —á—Ç–æ –Ω—É–∂–Ω–æ –∑–Ω–∞—Ç—å –∑–∞—Ä–∞–Ω–µ–µ
        'cultural_references': int,
        'intertextuality': float,  # –æ—Ç—Å—ã–ª–∫–∏ –∫ –¥—Ä—É–≥–∏–º —Ç–µ–∫—Å—Ç–∞–º
        'symbolism_density': float,
        'allegory_level': float,  # 0=–ø—Ä—è–º–æ–µ –ø–æ–≤–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ, 1=—á–∏—Å—Ç–∞—è –∞–ª–ª–µ–≥–æ—Ä–∏—è
    },
    
    'world_view': {
        'optimism_pessimism': float,  # -1 to 1
        'determinism_free_will': float,  # -1 (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏–∑–º) to 1 (—Å–≤–æ–±–æ–¥–∞ –≤–æ–ª–∏)
        'individualism_collectivism': float,
        'idealism_pragmatism': float,
        'tradition_progress': float
    },
    
    'questions_raised': List[Dict],  # [{'question': str, 'answer_provided': bool, 'ambiguity': float}]
    
    'provocative_elements': {
        'challenges_beliefs': List[str],  # –∫–∞–∫–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è –æ—Å–ø–∞—Ä–∏–≤–∞–µ—Ç
        'cognitive_dissonance_potential': float,  # 1-10
        'paradigm_shift_potential': float,  # —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∏–∑–º–µ–Ω–∏—Ç—å –º–∏—Ä–æ–≤–æ–∑–∑—Ä–µ–Ω–∏–µ
        'controversy_level': float  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Ä–Ω–æ
    }
}
```

#### 1.9 –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
```python
emotional_parameters = {
    'emotional_arc': {
        'type': str,  # rags_to_riches, tragedy, man_in_hole, etc (Vonnegut/Reagan)
        'intensity_curve': List[float],  # —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å –ø–æ –≤—Ä–µ–º–µ–Ω–∏
        'volatility': float,  # 1-10, —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å vs –∏–∑–º–µ–Ω—á–∏–≤–æ—Å—Ç—å
        'final_tone': float,  # -1 (—Ç—Ä–∞–≥–µ–¥–∏—è) to 1 (–æ–ø—Ç–∏–º–∏–∑–º)
        'emotional_resolution': float  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑—Ä–µ—à–µ–Ω—ã —ç–º–æ—Ü–∏–∏ –∫ –∫–æ–Ω—Ü—É
    },
    
    'emotional_range': {
        'emotions_present': List[str],  # ['joy', 'sadness', 'fear', 'anger', etc]
        'emotional_diversity': float,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–∞–∑–Ω—ã—Ö —ç–º–æ—Ü–∏–π
        'dominant_emotion': str,
        'emotional_balance': Dict[str, float],  # –¥–æ–ª—è –∫–∞–∂–¥–æ–π —ç–º–æ—Ü–∏–∏
    },
    
    'emotional_intensity': {
        'overall': float,  # 1-10
        'peak_intensity': float,  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
        'average_intensity': float,
        'low_points': float  # –µ—Å—Ç—å –ª–∏ –º–æ–º–µ–Ω—Ç—ã –æ—Ç–¥—ã—Ö–∞
    },
    
    'emotional_complexity': {
        'ambivalence': float,  # –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤—ã–µ —ç–º–æ—Ü–∏–∏ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
        'nuance': float,  # —Ç–æ–Ω–∫–æ—Å—Ç—å —ç–º–æ—Ü–∏–π
        'emotional_realism': float  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã —ç–º–æ—Ü–∏–∏
    },
    
    'catharsis_potential': float,  # 1-10, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–º—É –æ—á–∏—â–µ–Ω–∏—é
    
    'atmosphere': {
        'primary': str,  # 'dark', 'light', 'melancholic', 'tense', etc
        'consistency': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å—Ç–∞–±–∏–ª—å–Ω–∞ –∞—Ç–º–æ—Å—Ñ–µ—Ä–∞
        'immersiveness': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–≥—Ä—É–∂–∞–µ—Ç
        'oppressiveness': float  # –¥–ª—è dark –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤
    }
}
```

#### 1.10 –¢—Ä–æ–ø—ã (–¥–µ—Ç–∞–ª—å–Ω–æ)
```python
trope_analysis = {
    'tropes_used': List[Dict],
    # –ö–∞–∂–¥—ã–π —Ç—Ä–æ–ø:
    {
        'trope_id': str,
        'trope_name': str,
        'category': str,  # plot_device, character_archetype, narrative_technique, etc
        
        # –ö–†–ò–¢–ò–ß–ù–û: —Ç–∏–ø –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        'usage_type': ['straight', 'deconstruction', 'subversion', 'reconstruction', 'meta'],
        
        # –ö–∞—á–µ—Å—Ç–≤–æ —Ä–∞–±–æ—Ç—ã —Å —Ç—Ä–æ–ø–æ–º
        'awareness': float,  # 1-10, –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å –∞–≤—Ç–æ—Ä–∞
        'execution': float,  # 1-10, –∫–∞—á–µ—Å—Ç–≤–æ –∏—Å–ø–æ–ª–Ω–µ–Ω–∏—è
        'originality': float,  # 1-10, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–≤–µ–∂–æ
        'centrality': float,  # 0-1, –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–∞–∂–µ–Ω –¥–ª—è –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è
        
        # –í–ª–∏—è–Ω–∏–µ –Ω–∞ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ
        'cognitive_load': float,  # 1-10, –Ω–∞–≥—Ä—É–∑–∫–∞ –Ω–∞ –∑—Ä–∏—Ç–µ–ª—è
        'expectation_handling': str,  # 'fulfills', 'subverts', 'complicates'
        'genre_dependency': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∑–Ω–∞–Ω–∏—è –∂–∞–Ω—Ä–∞
        
        # –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ—Å—Ç—å —Ç—Ä–æ–ø–∞
        'transformation_potential': float,  # 1-10
        'meta_commentary': bool,  # –µ—Å—Ç—å –ª–∏ –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –æ —Å–∞–º–æ–º —Ç—Ä–æ–ø–µ
        'cultural_impact': float  # –ø–æ–≤–ª–∏—è–ª –ª–∏ –Ω–∞ –∫—É–ª—å—Ç—É—Ä—É
    },
    
    # –ú–µ—Ç–∞-–∞–Ω–∞–ª–∏–∑ —Ä–∞–±–æ—Ç—ã —Å —Ç—Ä–æ–ø–∞–º–∏
    'trope_engagement': {
        'meta_awareness': float,  # 1-10, –æ–±—â–∏–π —É—Ä–æ–≤–µ–Ω—å –º–µ—Ç–∞–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç–∏
        'overall_skill': float,  # 1-10, –æ–±—â–µ–µ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–æ
        'transformation_depth': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –≥–ª—É–±–æ–∫–æ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä—É–µ—Ç
        'genre_expectations': str,  # –∫–∞–∫–∏–µ –∂–∞–Ω—Ä–æ–≤—ã–µ –æ–∂–∏–¥–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç
        'audience_sophistication_required': float  # 1-10, —Ç—Ä–µ–±—É–µ–º—ã–π —É—Ä–æ–≤–µ–Ω—å
    },
    
    # –†–∞–±–æ—Ç–∞ —Å –æ–∂–∏–¥–∞–Ω–∏—è–º–∏
    'expectation_work': {
        'subverted_patterns': List[str],
        'reinforced_patterns': List[str],
        'meta_commentary_present': bool,
        'irony_level': float,  # 1-10
        'self_awareness': float  # –æ—Å–æ–∑–Ω–∞–Ω–∏–µ —Å–µ–±—è –∫–∞–∫ —Ç–µ–∫—Å—Ç–∞
    }
}
```

#### 1.11 –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
```python
cognitive_operations = {
    'operations_activated': List[Dict],
    # 8 –æ–ø–µ—Ä–∞—Ü–∏–π –∏–∑ –º–µ—Ç–∞—Å–∏—Å—Ç–µ–º—ã:
    {
        'operation': ['Pause', 'Observation', 'Decomposition', 'Linkage', 
                     'Separation', 'Testing', 'Integration', 'Reflection'],
        'intensity': float,  # 1-10, –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω–æ –∞–∫—Ç–∏–≤–∏—Ä—É–µ—Ç
        'frequency': float,  # –∫–∞–∫ —á–∞—Å—Ç–æ —Ç—Ä–µ–±—É–µ—Ç—Å—è
        'difficulty': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–ª–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å
        'guidance': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–∞—Ç–µ—Ä–∏–∞–ª –ø–æ–º–æ–≥–∞–µ—Ç (1=—è–≤–Ω–æ, 10=–Ω–µ—è–≤–Ω–æ)
    },
    
    'cognitive_style_match': {
        'visual': float,  # 0-1, –ø–æ–¥—Ö–æ–¥–∏—Ç –≤–∏–∑—É–∞–ª–∞–º
        'analytical': float,  # –ø–æ–¥—Ö–æ–¥–∏—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫–∞–º
        'empathetic': float,  # –ø–æ–¥—Ö–æ–¥–∏—Ç —ç–º–ø–∞—Ç–∞–º
        'kinesthetic': float  # –ø–æ–¥—Ö–æ–¥–∏—Ç –∫–∏–Ω–µ—Å—Ç–µ—Ç–∏–∫–∞–º
    },
    
    'mental_models': {
        'challenges_models': List[str],  # –∫–∞–∫–∏–µ –º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—Å–ø–∞—Ä–∏–≤–∞–µ—Ç
        'introduces_models': List[str],  # –∫–∞–∫–∏–µ –≤–≤–æ–¥–∏—Ç
        'requires_models': List[str]  # –∫–∞–∫–∏–µ –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ—Ç
    }
}
```

#### 1.12 –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª (–¥–µ—Ç–∞–ª—å–Ω–æ)
```python
transformative_potential = {
    # –ë–∞–∑–æ–≤—ã–π score
    'transformative_score': float,  # 0-10
    
    # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    'components': {
        'cognitive_dissonance': float,  # 0-1, —Å–æ–∑–¥–∞—ë—Ç –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –¥–∏—Å—Å–æ–Ω–∞–Ω—Å
        'perspective_shifting': float,  # 0-1, –º–µ–Ω—è–µ—Ç —Ç–æ—á–∫—É –∑—Ä–µ–Ω–∏—è
        'identification_distancing': float,  # 0-1, –±–∞–ª–∞–Ω—Å –±–ª–∏–∑–æ—Å—Ç–∏/–æ—Ç—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è
        'progressive_revelation': float,  # 0-1, –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–µ —Ä–∞—Å–∫—Ä—ã—Ç–∏–µ
        'metacognitive_awareness': float,  # 0-1, —Ä–µ—Ñ–ª–µ–∫—Å–∏—è –æ –º—ã—à–ª–µ–Ω–∏–∏
        'multiple_perspectives': float,  # 0-1, –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ—Å—Ç—å —Ç–æ—á–µ–∫ –∑—Ä–µ–Ω–∏—è
        'ambiguity_tolerance': float,  # —Ç—Ä–µ–±—É–µ–º–∞—è —Ç–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –Ω–µ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–æ—Å—Ç–∏
        'complexity_handling': float  # —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç–∞—Ç—å —Å–æ —Å–ª–æ–∂–Ω–æ—Å—Ç—å—é
    },
    
    # –ú–µ—Ö–∞–Ω–∏–∑–º—ã —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    'mechanisms': List[str],  # ['challenges_assumptions', 'introduces_paradox', etc]
    
    # –¢—Ä–∞–µ–∫—Ç–æ—Ä–∏—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    'transformation_journey': {
        'entry_barrier': float,  # 1-10, —Å–ª–æ–∂–Ω–æ—Å—Ç—å –≤—Ö–æ–¥–∞
        'peak_challenge': float,  # –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        'integration_support': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–º–æ–≥–∞–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å
        'lasting_impact_potential': float  # –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ —ç—Ñ—Ñ–µ–∫—Ç–∞
    },
    
    # –¶–µ–ª–µ–≤—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
    'target_changes': {
        'belief_changes': List[str],  # –∫–∞–∫–∏–µ —É–±–µ–∂–¥–µ–Ω–∏—è –º–µ–Ω—è–µ—Ç
        'skill_development': List[str],  # –∫–∞–∫–∏–µ –Ω–∞–≤—ã–∫–∏ —Ä–∞–∑–≤–∏–≤–∞–µ—Ç
        'emotional_growth': List[str],  # —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–æ—Å—Ç
        'worldview_expansion': List[str]  # —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –º–∏—Ä–æ–≤–æ–∑–∑—Ä–µ–Ω–∏—è
    },
    
    'rewatch_value': float,  # 1-10, —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–∞
    'discussion_potential': float  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –ø—Ä–æ–≤–æ—Ü–∏—Ä—É–µ—Ç –æ–±—Å—É–∂–¥–µ–Ω–∏–µ
}
```

#### 1.13 –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –≤–ª–∏—è–Ω–∏—è
```python
cultural_context = {
    'historical_context': {
        'period': str,
        'cultural_moment': str,
        'zeitgeist_reflection': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –æ—Ç—Ä–∞–∂–∞–µ—Ç –¥—É—Ö –≤—Ä–µ–º–µ–Ω–∏
        'period_accuracy': float  # –¥–ª—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö
    },
    
    'influences': {
        'influenced_by': List[Dict],  # [{'title': str, 'type': str, 'strength': float}]
        'influences_on': List[str],  # —á—Ç–æ –ø–æ–≤–ª–∏—è–ª–æ –Ω–∞
        'part_of_movement': str,  # –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–Ω–æ–µ/–∫–∏–Ω–µ–º–∞—Ç–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ –¥–≤–∏–∂–µ–Ω–∏–µ
        'genre_defining': bool,  # –æ–ø—Ä–µ–¥–µ–ª–∏–ª–æ –∂–∞–Ω—Ä
    },
    
    'innovation': {
        'technical': List[str],  # —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
        'narrative': List[str],  # –Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω—ã–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
        'thematic': List[str],  # —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
        'overall_innovation': float  # 1-10
    },
    
    'cultural_impact': {
        'mainstream_penetration': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –≤–æ—à–ª–æ –≤ –∫—É–ª—å—Ç—É—Ä—É
        'critical_acclaim': float,
        'audience_reception': float,
        'longevity': float,  # –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º
        'quotability': float,  # —Ü–∏—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å
        'meme_potential': float  # –º–µ–º–æ–≥–µ–Ω–Ω–æ—Å—Ç—å
    },
    
    'awards': List[Dict],  # –Ω–∞–≥—Ä–∞–¥—ã –∏ –Ω–æ–º–∏–Ω–∞—Ü–∏–∏
    
    'canonical_status': float  # 0-1, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–∞–Ω–æ–Ω–∏—á–µ–Ω
}
```

#### 1.14 –ê—É–¥–∏—Ç–æ—Ä–∏—è –∏ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
```python
audience_parameters = {
    'target_audience': {
        'age_recommendation': str,  # '16+', '18+', etc
        'education_level': str,  # —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π —É—Ä–æ–≤–µ–Ω—å –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è
        'cultural_knowledge': List[str],  # —Ç—Ä–µ–±—É–µ–º—ã–µ –∫—É–ª—å—Ç—É—Ä–Ω—ã–µ –∑–Ω–∞–Ω–∏—è
        'genre_literacy_required': Dict[str, float],  # {genre: level}
    },
    
    'accessibility': {
        'reading_level': str,  # –¥–ª—è –∫–Ω–∏–≥
        'requires_annotations': bool,
        'requires_guide': bool,
        'suitable_for_audiobook': bool,  # –¥–ª—è –∫–Ω–∏–≥
        'suitable_for_background': bool,  # –¥–ª—è —Ñ–∏–ª—å–º–æ–≤
        'attention_required': float  # 1-10
    },
    
    'content_warnings': {
        'trigger_warnings': List[str],
        'sensitive_topics': List[str],
        'intensity_by_type': Dict[str, float]  # {'violence': 7, 'sex': 3}
    },
    
    'user_experience': {
        'typical_response': str,  # –æ–ø–∏—Å–∞–Ω–∏–µ —Ç–∏–ø–∏—á–Ω–æ–π —Ä–µ–∞–∫—Ü–∏–∏
        'emotional_journey': str,
        'post_consumption_state': str,
        'processing_time': str,  # —Å–∫–æ–ª—å–∫–æ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–≤–∞—Ä–∏—Ç—å
        'difficulty_curve': str  # –∫–∞–∫ –º–µ–Ω—è–µ—Ç—Å—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    },
    
    'context_dependency': {
        'best_for': List[str],  # –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã
        'not_recommended_for': List[str],
        'mood_dependency': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
        'time_of_day': str  # –∫–æ–≥–¥–∞ –ª—É—á—à–µ —Å–º–æ—Ç—Ä–µ—Ç—å
    }
}
```

#### 1.15 –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
```python
practical_metadata = {
    'time_investment': {
        'estimated_hours': float,
        'can_be_paused': bool,
        'episodic_structure': bool,  # –º–æ–∂–Ω–æ –ª–∏ –ø–æ —á–∞—Å—Ç—è–º
        'rewatch_required': bool
    },
    
    'difficulty_factors': {
        'difficulty_level': float,  # 1-10, –æ–±—â–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        'entry_difficulty': float,  # —Å–ª–æ–∂–Ω–æ—Å—Ç—å –Ω–∞—á–∞–ª–∞
        'sustained_difficulty': float,  # —Å—Ç–∞–±–∏–ª—å–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        'peaks': List[float],  # –ø–∏–∫–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
        
        'difficulty_sources': {
            'structural': float,  # –∏–∑-–∑–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
            'linguistic': float,  # –∏–∑-–∑–∞ —è–∑—ã–∫–∞
            'conceptual': float,  # –∏–∑-–∑–∞ –∫–æ–Ω—Ü–µ–ø—Ü–∏–π
            'cultural': float,  # –∏–∑-–∑–∞ –∫—É–ª—å—Ç—É—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            'emotional': float  # –∏–∑-–∑–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏
        }
    },
    
    'prerequisite_knowledge': {
        'required': List[str],  # –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ
        'recommended': List[str],  # –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–µ
        'enhancing': List[str]  # –æ–±–æ–≥–∞—â–∞—é—â–µ–µ –≤–æ—Å–ø—Ä–∏—è—Ç–∏–µ
    },
    
    'gateway_potential': {
        'gateway_to': List[str],  # –∫ —á–µ–º—É –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç –ø—É—Ç—å
        'preparation_for': List[str],  # –∫ —á–µ–º—É –≥–æ—Ç–æ–≤–∏—Ç
        'requires_first': List[str]  # —á—Ç–æ –Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞
    }
}
```

---

### 2. –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ò (–º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å)

#### 2.1 –î–µ–º–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
```python
user_demographics = {
    'age': int,
    'education_level': str,
    'occupation': str,
    'languages': List[str],
    'cultural_background': List[str],
    'location': str
}
```

#### 2.2 –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
```python
cognitive_profile = {
    # –û—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∏–ª—å
    'primary_cognitive_style': ['visual', 'analytical', 'empathetic', 'kinesthetic'],
    'secondary_styles': List[str],
    'style_flexibility': float,  # 0-1, –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–∂–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥—Ä—É–≥–∏–º–∏ —Å—Ç–∏–ª—è–º–∏
    
    # –£—Ä–æ–≤–Ω–∏ —Ä–∞–∑–≤–∏—Ç–∏—è
    'current_complexity_comfort': float,  # 1-10, –∫–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    'current_max_difficulty': float,  # 1-10, –º–∞–∫—Å–∏–º—É–º —á—Ç–æ –º–æ–∂–µ—Ç –æ—Å–∏–ª–∏—Ç—å
    'growth_rate': float,  # —Å–∫–æ—Ä–æ—Å—Ç—å —Ä–æ—Å—Ç–∞ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    'plateau_periods': List[Dict],  # –ø–µ—Ä–∏–æ–¥—ã –∑–∞—Å—Ç–æ—è
    
    # –ú–µ—Ç–∞–∫–æ–≥–Ω–∏—Ü–∏—è
    'meta_awareness_level': float,  # 1-10, —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ —Ä–µ—Ñ–ª–µ–∫—Å–∏–∏
    'critical_thinking_skills': float,  # 1-10
    'abstract_thinking': float,  # 1-10
    'pattern_recognition': float,  # 1-10
    
    # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
    'operations_proficiency': Dict[str, float],  # {operation: level}
    'operations_preference': Dict[str, float],  # –∫–∞–∫–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç
    
    # –û–±—É—á–µ–Ω–∏–µ
    'learning_style': ['visual', 'auditory', 'reading', 'kinesthetic'],
    'preferred_pace': str,  # 'slow', 'moderate', 'fast'
    'tolerance_for_ambiguity': float,  # 1-10
    'need_for_closure': float  # 1-10
}
```

#### 2.3 –ñ–∞–Ω—Ä–æ–≤–∞—è –≥—Ä–∞–º–æ—Ç–Ω–æ—Å—Ç—å
```python
genre_literacy = {
    'genres': Dict[str, Dict],
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∂–∞–Ω—Ä–∞:
    {
        'literacy_level': float,  # 1-10, –∑–Ω–∞–Ω–∏–µ –∫–æ–Ω–≤–µ–Ω—Ü–∏–π
        'exposure': int,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö
        'recency': datetime,  # –∫–æ–≥–¥–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —Ä–∞–∑
        'preference': float,  # 1-10, –Ω–∞—Å–∫–æ–ª—å–∫–æ –ª—é–±–∏—Ç
        'expertise_areas': List[str],  # –≤ —á—ë–º –æ—Å–æ–±–µ–Ω–Ω–æ —Ä–∞–∑–±–∏—Ä–∞–µ—Ç—Å—è
        'knowledge_gaps': List[str]  # —á—Ç–æ –Ω–µ –∑–Ω–∞–µ—Ç
    },
    
    # –ú–µ—Ç–∞-–∂–∞–Ω—Ä–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ
    'cross_genre_understanding': float,  # –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –∂–∞–Ω—Ä–∞–º–∏
    'genre_evolution_awareness': float,  # –∑–Ω–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ –∂–∞–Ω—Ä–æ–≤
    'subgenre_sensitivity': float  # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫ —Ç–æ–Ω–∫–∏–º —Ä–∞–∑–ª–∏—á–∏—è–º
}
```

#### 2.4 –†–∞–±–æ—Ç–∞ —Å —Ç—Ä–æ–ø–∞–º–∏
```python
trope_profile = {
    # –ü—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
    'trope_preferences': Dict[str, Dict],
    # –î–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç—Ä–æ–ø–∞:
    {
        'overall_rating': float,  # —Å—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —Å —Ç—Ä–æ–ø–æ–º
        'by_usage_type': Dict[str, float],  # {'straight': 6, 'deconstruction': 9}
        'by_quality': Dict[str, float],  # –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–µ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É
        'exposure_count': int,  # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –≤—Å—Ç—Ä–µ—á–∞–ª
        'recent_trend': List[float],  # –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 –æ—Ü–µ–Ω–æ–∫
        'fatigue_level': float,  # 0-10, —É—Å—Ç–∞–ª–æ—Å—Ç—å
        'interest_level': float  # 0-10, –∏–Ω—Ç–µ—Ä–µ—Å
    },
    
    # –£—Å—Ç–∞–ª–æ—Å—Ç—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è)
    'trope_fatigue': {
        'automatic': Dict[str, float],  # –∞–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª—ë–Ω–Ω–∞—è —É—Å—Ç–∞–ª–æ—Å—Ç—å
        'explicit': Dict[str, float],  # —è–≤–Ω–æ —É–∫–∞–∑–∞–Ω–Ω–∞—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º
        'decay_rate': float  # –∫–∞–∫ –±—ã—Å—Ç—Ä–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç —É—Å—Ç–∞–ª–æ—Å—Ç—å
    },
    
    # –ò–Ω—Ç–µ—Ä–µ—Å (—è–≤–Ω—ã–π)
    'trope_interest': Dict[str, float],
    
    # –ú–µ—Ç–∞–ø–æ–Ω–∏–º–∞–Ω–∏–µ
    'meta_trope_awareness': float,  # 1-10, –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Ç—Ä–æ–ø–æ–≤ –∫–∞–∫ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
    'favorite_deconstructions': List[str],  # –ª—é–±–∏–º—ã–µ –¥–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
    'appreciation_for_meta': float  # –ª—é–±–æ–≤—å –∫ –º–µ—Ç–∞–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—é
}
```

#### 2.5 –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è
```python
thematic_preferences = {
    'themes': Dict[str, Dict],
    # –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã:
    {
        'interest_level': float,  # 1-10
        'exploration_depth': str,  # 'surface', 'moderate', 'deep'
        'perspective_preference': str,  # 'traditional', 'challenging', 'both'
        'emotional_readiness': float  # –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Ç–µ–º–µ
    },
    
    'philosophical_interests': {
        'epistemology': float,
        'metaphysics': float,
        'ethics': float,
        'existentialism': float,
        'political_philosophy': float,
        'aesthetics': float
    },
    
    'worldview_alignment': {
        'optimism_pessimism': float,  # -1 to 1
        'determinism_free_will': float,
        'individualism_collectivism': float,
        'idealism_pragmatism': float
    },
    
    'challenge_appetite': {
        'belief_challenging': float,  # –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –æ—Å–ø–∞—Ä–∏–≤–∞–Ω–∏—é —É–±–µ–∂–¥–µ–Ω–∏–π
        'cognitive_dissonance_tolerance': float,
        'paradigm_shift_openness': float
    }
}
```

#### 2.6 –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
```python
emotional_profile = {
    'emotional_preferences': {
        'preferred_emotions': List[str],
        'avoided_emotions': List[str],
        'emotional_intensity_preference': float,  # 1-10
        'emotional_range_preference': str  # 'focused', 'varied'
    },
    
    'emotional_regulation': {
        'max_emotional_intensity': float,  # 1-10, –º–∞–∫—Å–∏–º—É–º —á—Ç–æ –º–æ–∂–µ—Ç –≤—ã–Ω–µ—Å—Ç–∏
        'recovery_time_needed': str,  # 'none', 'short', 'long'
        'catharsis_seeking': float,  # –∏—â–µ—Ç –ª–∏ –∫–∞—Ç–∞—Ä—Å–∏—Å
        'emotional_resilience': float  # 1-10
    },
    
    'mood_patterns': {
        'current_mood': str,
        'mood_history': List[Dict],  # –∏—Å—Ç–æ—Ä–∏—è –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–π
        'mood_influence_on_choices': float  # –Ω–∞—Å–∫–æ–ª—å–∫–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –≤–ª–∏—è–µ—Ç
    },
    
    'triggers': {
        'avoid_triggers': List[str],  # —Å—Ç—Ä–æ–≥–∏–µ —Ç—Ä–∏–≥–≥–µ—Ä—ã
        'sensitive_topics': List[str],  # —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–º—ã
        'trigger_severity': Dict[str, float],  # —Å–µ—Ä—å—ë–∑–Ω–æ—Å—Ç—å —Ç—Ä–∏–≥–≥–µ—Ä–∞
        'context_dependent_triggers': Dict[str, List[str]]  # –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    }
}
```

#### 2.7 –ò—Å—Ç–æ—Ä–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
```python
interaction_history = {
    'ratings': List[Dict],
    # –ö–∞–∂–¥–∞—è –æ—Ü–µ–Ω–∫–∞:
    {
        'material_id': int,
        'rating': float,  # 0-10
        'timestamp': datetime,
        
        # –î–µ—Ç–∞–ª–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è
        'personal_transformation_score': float,  # —Å—É–±—ä–µ–∫—Ç–∏–≤–Ω–∞—è —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è
        'difficulty_experienced': float,  # –≤–æ—Å–ø—Ä–∏–Ω—è—Ç–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
        'emotional_impact': float,  # —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –≤–æ–∑–¥–µ–π—Å—Ç–≤–∏–µ
        'intellectual_satisfaction': float,
        'entertainment_value': float,
        
        # –ö–æ–Ω—Ç–µ–∫—Å—Ç
        'viewing_context': str,  # –∫–æ–≥–¥–∞/–≥–¥–µ/–∫–∞–∫ —Å–º–æ—Ç—Ä–µ–ª
        'mood_before': str,
        'mood_after': str,
        'completion_rate': float,  # 0-1, –¥–æ—Å–º–æ—Ç—Ä–µ–ª –ª–∏ –¥–æ –∫–æ–Ω—Ü–∞
        'rewatch_probability': float,
        
        # –ó–∞–º–µ—Ç–∫–∏
        'notes': str,
        'favorite_moments': List[str],
        'criticisms': List[str]
    },
    
    # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø–æ–≤–µ–¥–µ–Ω–∏—è
    'behavioral_patterns': {
        'binge_tendency': float,  # —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –ø—Ä–æ—Å–º–æ—Ç—Ä—É —Å–µ—Ä–∏—è–º–∏
        'completion_rate_avg': float,  # % –¥–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö
        'rating_bias': float,  # —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –∫ –∑–∞–≤—ã—à–µ–Ω–∏—é/–∑–∞–Ω–∏–∂–µ–Ω–∏—é
        'rating_variance': float,  # —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ—Ü–µ–Ω–æ–∫
        'discovery_method': Dict[str, int],  # –∫–∞–∫ –æ–±—ã—á–Ω–æ –Ω–∞—Ö–æ–¥–∏—Ç –º–∞—Ç–µ—Ä–∏–∞–ª—ã
    },
    
    # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    'temporal_patterns': {
        'active_hours': List[int],  # –∫–æ–≥–¥–∞ –æ–±—ã—á–Ω–æ —Å–º–æ—Ç—Ä–∏—Ç
        'session_duration_avg': float,  # —Å—Ä–µ–¥–Ω—è—è –ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–µ—Å—Å–∏–∏
        'frequency': str,  # 'daily', 'weekly', etc
        'seasonal_patterns': Dict[str, float]  # –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –ø–æ —Å–µ–∑–æ–Ω–∞–º
    }
}
```

#### 2.8 –°–æ—Ü–∏–∞–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
```python
social_context = {
    'social_consumption': {
        'prefers_solo_vs_group': float,  # -1 (solo) to 1 (group)
        'discussion_tendency': float,  # —Å–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –æ–±—Å—É–∂–¥–∞—Ç—å
        'recommendation_receptivity': float,  # –Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–ª—É—à–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        'influence_from_peers': float
    },
    
    'community_engagement': {
        'online_community_participation': float,
        'review_writing': bool,
        'discussion_forum_activity': float,
        'social_media_sharing': float
    },
    
    'taste_leaders': List[str],  # –Ω–∞ —á–µ–π –≤–∫—É—Å –æ—Ä–∏–µ–Ω—Ç–∏—Ä—É–µ—Ç—Å—è
    'similar_users': List[int]  # –ø–æ—Ö–æ–∂–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ (–¥–ª—è collaborative filtering)
}
```

#### 2.9 –¶–µ–ª–∏ –∏ –º–æ—Ç–∏–≤–∞—Ü–∏—è
```python
goals_motivation = {
    'primary_goals': List[str],  # ['intellectual_growth', 'entertainment', 'education', etc]
    'goal_weights': Dict[str, float],  # –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–π —Ü–µ–ª–∏
    
    'learning_goals': {
        'target_skills': List[str],
        'target_knowledge_areas': List[str],
        'target_complexity_level': float,  # –∫—É–¥–∞ —Ö–æ—á–µ—Ç –¥–æ—Ä–∞—Å—Ç–∏
        'timeline': str  # –∫–æ–≥–¥–∞ —Ö–æ—á–µ—Ç –¥–æ—Å—Ç–∏—á—å
    },
    
    'exploration_strategy': {
        'comfort_zone_preference': float,  # 0=–ª—é–±–∏—Ç –∫–æ–º—Ñ–æ—Ä—Ç, 1=–ª—é–±–∏—Ç challenge
        'novelty_seeking': float,  # —Ç—è–≥–∞ –∫ –Ω–æ–≤–æ–º—É
        'depth_vs_breadth': float,  # -1 (–≥–ª—É–±–∏–Ω–∞) to 1 (—à–∏—Ä–æ—Ç–∞)
        'risk_tolerance': float  # –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–º
    },
    
    'current_focus': {
        'active_explorations': List[str],  # —á—Ç–æ —Å–µ–π—á–∞—Å –∏–∑—É—á–∞–µ—Ç
        'preparation_for': str,  # –∫ —á–µ–º—É –≥–æ—Ç–æ–≤–∏—Ç—Å—è
        'avoiding': List[str]  # —á—Ç–æ —Å–µ–π—á–∞—Å –∏–∑–±–µ–≥–∞–µ—Ç
    }
}
```

#### 2.10 –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã
```python
time_resources = {
    'available_time': {
        'hours_per_week': float,
        'session_length_preference': str,  # 'short', 'medium', 'long'
        'can_do_long_form': bool,
        'prefers_episodic': bool
    },
    
    'scheduling': {
        'flexible_schedule': bool,
        'preferred_times': List[str],
        'binge_capability': bool
    },
    
    'commitment_level': {
        'willing_to_invest': str,  # 'low', 'medium', 'high'
        'patience_for_slow_burn': float,
        'persistence': float  # –¥–æ–≤–æ–¥–∏—Ç –ª–∏ –¥–æ –∫–æ–Ω—Ü–∞ —Å–ª–æ–∂–Ω–æ–µ
    }
}
```

---

### 3. –ö–û–ù–¢–ï–ö–°–¢–£–ê–õ–¨–ù–´–ï –î–ê–ù–ù–´–ï

#### 3.1 –°–µ—Å—Å–∏–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
```python
session_context = {
    'current_session': {
        'time_of_day': str,
        'day_of_week': str,
        'device': str,
        'location_type': str,  # 'home', 'commute', 'travel'
        'alone_or_with_others': str,
        'ambient_conditions': str  # 'quiet', 'noisy', etc
    },
    
    'user_state': {
        'energy_level': float,  # 1-10
        'focus_capacity': float,  # 1-10
        'emotional_state': str,
        'stress_level': float,
        'time_pressure': float  # –µ—Å—Ç—å –ª–∏ —Å–ø–µ—à–∫–∞
    },
    
    'immediate_needs': {
        'relaxation': float,
        'stimulation': float,
        'escape': float,
        'learning': float,
        'inspiration': float
    }
}
```

#### 3.2 –í–Ω–µ—à–Ω–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã
```python
external_factors = {
    'seasonal': {
        'season': str,
        'weather': str,
        'holidays': bool
    },
    
    'cultural_events': {
        'current_events': List[str],
        'trending_topics': List[str],
        'cultural_moments': List[str]
    },
    
    'personal_events': {
        'life_changes': List[str],
        'recent_experiences': List[str],
        'upcoming_events': List[str]
    }
}
```

---

## üßÆ –ú–ê–ö–°–ò–ú–ê–õ–¨–ù–û –ü–û–õ–ù–´–ô –ê–õ–ì–û–†–ò–¢–ú

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   USER INPUT & CONTEXT                       ‚îÇ
‚îÇ  - Explicit preferences                                      ‚îÇ
‚îÇ  - Interaction history                                       ‚îÇ
‚îÇ  - Current session context                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              FEATURE EXTRACTION & EMBEDDING                  ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îÇ
‚îÇ  ‚îÇ   Material  ‚îÇ  ‚îÇ    User     ‚îÇ  ‚îÇ  Context    ‚îÇ        ‚îÇ
‚îÇ  ‚îÇ  Embeddings ‚îÇ  ‚îÇ  Embeddings ‚îÇ  ‚îÇ  Embeddings ‚îÇ        ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ            MULTI-COMPONENT SCORING                           ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ   Trope   ‚îÇ ‚îÇ  Content  ‚îÇ ‚îÇStructural ‚îÇ ‚îÇTransform  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Based   ‚îÇ ‚îÇ   Based   ‚îÇ ‚îÇ   Based   ‚îÇ ‚îÇ   Based   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ        ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ          ‚îÇ
‚îÇ        ‚ñº             ‚ñº             ‚ñº             ‚ñº          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇCollab     ‚îÇ ‚îÇPopularity ‚îÇ ‚îÇ  Context  ‚îÇ ‚îÇ  Serendip ‚îÇ  ‚îÇ
‚îÇ  ‚îÇFiltering  ‚îÇ ‚îÇ   Based   ‚îÇ ‚îÇ   Aware   ‚îÇ ‚îÇ    ity    ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ        ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ          ‚îÇ
‚îÇ        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                            ‚îÇ                                 ‚îÇ
‚îÇ                            ‚ñº                                 ‚îÇ
‚îÇ                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ                    ‚îÇ  Ensemble &   ‚îÇ                        ‚îÇ
‚îÇ                    ‚îÇ  Calibration  ‚îÇ                        ‚îÇ
‚îÇ                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                POST-PROCESSING & FILTERING                   ‚îÇ
‚îÇ  - Safety filters (triggers)                                ‚îÇ
‚îÇ  - Diversity optimization                                    ‚îÇ
‚îÇ  - Zone of Proximal Development                              ‚îÇ
‚îÇ  - Explanation generation                                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚îÇ
                   ‚ñº
              RECOMMENDATIONS
```

---

### Component 1: Trope-Based Scoring (25%)

```python
def trope_based_score(material, user, weight=0.25):
    """
    –í–µ—Ä—Å–∏—è 3.0 —Å ML embeddings
    """
    score = 0
    details = []
    
    # 1. EMBEDDING SIMILARITY
    # –í–µ–∫—Ç–æ—Ä —Ç—Ä–æ–ø–æ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    material_trope_embedding = generate_trope_embedding(material)
    
    # –í–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_trope_embedding = generate_user_trope_preference_embedding(user)
    
    # –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å
    embedding_similarity = cosine_similarity(
        material_trope_embedding,
        user_trope_embedding
    )
    
    score += embedding_similarity * 4.0  # max 4.0
    
    # 2. EXPLICIT TROPE MATCHING
    for trope in material.analyzed_tropes:
        # –ö–ª—é—á = tropeId + usageType + –∫–∞—á–µ—Å—Ç–≤–æ
        key = f"{trope.trope_id}_{trope.usage_type}_q{trope.execution//3}"
        
        # Preference –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        if key in user.trope_preferences:
            pref = user.trope_preferences[key]
            
            # –í–µ—Å–∞
            centrality_weight = trope.centrality
            quality_weight = trope.execution / 10.0
            
            # –í—Ä–µ–º–µ–Ω–Ω–æ–π decay
            time_decay = calculate_preference_decay(
                user.trope_preferences[key].last_updated
            )
            
            contribution = pref * centrality_weight * quality_weight * time_decay
            score += contribution * 0.3  # –¥–æ 3.0
            
            if pref >= 8:
                details.append(f"–í–∞–º –Ω—Ä–∞–≤–∏—Ç—Å—è {trope.name} ({trope.usage_type})")
        
        # –£—Å—Ç–∞–ª–æ—Å—Ç—å
        if trope.trope_id in user.trope_fatigue:
            fatigue = user.trope_fatigue[trope.trope_id]
            
            if trope.usage_type == 'straight':
                penalty = fatigue * centrality_weight * 0.3
                score -= penalty
                
                if fatigue >= 7:
                    details.append(f"‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–∞ —É—Å—Ç–∞–ª–æ—Å—Ç—å –æ—Ç {trope.name}")
            else:
                # –î–µ–∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏—è —Å–Ω–∏–∂–∞–µ—Ç —É—Å—Ç–∞–ª–æ—Å—Ç—å
                score += (fatigue / 10) * 0.2
    
    # 3. –ú–ï–¢–ê–ù–ê–†–†–ê–¢–ò–í–ù–û–ï –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï
    required_meta = material.trope_engagement.audience_sophistication_required
    user_meta = user.meta_awareness_level
    
    if abs(required_meta - user_meta) <= 1:
        score += 2.0  # –∏–¥–µ–∞–ª—å–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
        if required_meta == user_meta + 1:
            details.append("‚≠ê –ó–æ–Ω–∞ —Ä–æ—Å—Ç–∞ –º–µ—Ç–∞–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç–∏")
    elif required_meta > user_meta + 2:
        score -= 1.5
        details.append("–ú–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –º–µ—Ç–∞-–Ω–∞—Ä—Ä–∞—Ç–∏–≤–Ω–æ")
    
    # 4. –ö–õ–ê–°–¢–ï–†–ù–´–ô –ê–ù–ê–õ–ò–ó
    # –î–ª—è —Ç—Ä–æ–ø–æ–≤ –±–µ–∑ –∏—Å—Ç–æ—Ä–∏–∏
    unknown_tropes = [t for t in material.analyzed_tropes 
                      if f"{t.trope_id}_{t.usage_type}" not in user.trope_preferences]
    
    if unknown_tropes:
        clustered_score = predict_trope_preference_from_cluster(
            unknown_tropes,
            user
        )
        score += clustered_score * 0.5
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-10
    final_score = min(10, max(0, score))
    
    return {
        'score': final_score,
        'details': details,
        'confidence': calculate_confidence(user.trope_exposure_count)
    }


def generate_trope_embedding(material):
    """
    –°–æ–∑–¥–∞—ë—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç—Ä–æ–ø–æ–≤ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, FastText –∏–ª–∏ custom)
    """
    trope_vectors = []
    
    for trope in material.analyzed_tropes:
        # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        base_vector = trope_embedding_model.encode(trope.trope_id)
        usage_vector = usage_type_encoding[trope.usage_type]
        quality_scalar = trope.execution / 10.0
        centrality_scalar = trope.centrality
        
        # –í–∑–≤–µ—à–µ–Ω–Ω—ã–π –≤–µ–∫—Ç–æ—Ä
        weighted_vector = (
            base_vector * quality_scalar * centrality_scalar
        )
        trope_vectors.append(weighted_vector)
    
    # –ê–≥—Ä–µ–≥–∞—Ü–∏—è (–≤–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ)
    if trope_vectors:
        material_embedding = np.average(
            trope_vectors,
            weights=[t.centrality for t in material.analyzed_tropes],
            axis=0
        )
    else:
        material_embedding = np.zeros(embedding_dim)
    
    return material_embedding


def generate_user_trope_preference_embedding(user):
    """
    –°–æ–∑–¥–∞—ë—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    preference_vectors = []
    weights = []
    
    for trope_key, pref_data in user.trope_preferences.items():
        trope_id, usage_type = parse_trope_key(trope_key)
        
        base_vector = trope_embedding_model.encode(trope_id)
        usage_vector = usage_type_encoding[usage_type]
        
        # –í–µ—Å = —Ä–µ–π—Ç–∏–Ω–≥ √ó recency √ó confidence
        weight = (
            pref_data.rating *
            calculate_recency_weight(pref_data.last_updated) *
            calculate_confidence_weight(pref_data.sample_size)
        )
        
        preference_vectors.append(base_vector + usage_vector)
        weights.append(weight)
    
    if preference_vectors:
        user_embedding = np.average(
            preference_vectors,
            weights=weights,
            axis=0
        )
    else:
        # –•–æ–ª–æ–¥–Ω—ã–π —Å—Ç–∞—Ä—Ç - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π —Å—Ç–∏–ª—å
        user_embedding = cognitive_style_embedding[user.cognitive_style]
    
    return user_embedding
```

---

### Component 2: Content-Based Scoring (20%)

```python
def content_based_score(material, user, weight=0.20):
    """
    –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ, –∂–∞–Ω—Ä–æ–≤–æ–µ, —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ
    """
    score = 0
    details = []
    
    # 1. –¢–ï–ú–ê–¢–ò–ß–ï–°–ö–û–ï –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï
    theme_score = 0
    user_themes = extract_user_theme_preferences(user)
    material_themes = material.themes
    
    for theme in material_themes:
        if theme.id in user_themes:
            # –£—á–∏—Ç—ã–≤–∞–µ–º prominence —Ç–µ–º—ã –∏ –∏–Ω—Ç–µ—Ä–µ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            contribution = (
                theme.prominence *
                user_themes[theme.id].interest_level *
                theme.transformative_potential / 10
            )
            theme_score += contribution
            
            if user_themes[theme.id].interest_level >= 8:
                details.append(f"–ò–Ω—Ç–µ—Ä–µ—Å—É—é—â–∞—è —Ç–µ–º–∞: {theme.name}")
    
    score += min(3.0, theme_score)
    
    # 2. –ñ–ê–ù–†–û–í–û–ï –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï
    genre_score = 0
    
    for genre, weight in material.genre_blend.items():
        if genre in user.genre_preferences:
            user_pref = user.genre_preferences[genre]
            literacy = user.genre_literacy.get(genre, 5)
            
            # Bonus –µ—Å–ª–∏ —Ö–æ—Ä–æ—à–æ –∑–Ω–∞–µ—Ç –∂–∞–Ω—Ä
            literacy_bonus = (literacy / 10) * 0.5
            
            contribution = (
                user_pref *
                weight *
                (1 + literacy_bonus)
            )
            genre_score += contribution
    
    score += min(2.5, genre_score / len(material.genre_blend))
    
    # 3. –°–û–ó–î–ê–¢–ï–õ–¨ (—Ä–µ–∂–∏—Å—Å—ë—Ä/–∞–≤—Ç–æ—Ä)
    if material.creator in user.creator_preferences:
        creator_pref = user.creator_preferences[material.creator]
        score += min(1.5, creator_pref / 10 * 1.5)
        
        if creator_pref >= 8:
            details.append(f"–õ—é–±–∏–º—ã–π –∞–≤—Ç–æ—Ä: {material.creator}")
    
    # 4. –§–ò–õ–û–°–û–§–°–ö–ê–Ø –ë–õ–ò–ó–û–°–¢–¨
    philosophy_distance = calculate_worldview_distance(
        material.worldview,
        user.worldview_alignment
    )
    
    # –ë–ª–∏–∑–æ—Å—Ç—å worldview –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–ª—é—Å–æ–º –∏–ª–∏ –º–∏–Ω—É—Å–æ–º
    if user.challenge_appetite.belief_challenging < 5:
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –±–ª–∏–∑–∫–æ–µ –º–∏—Ä–æ–≤–æ–∑–∑—Ä–µ–Ω–∏–µ
        score += (1 - philosophy_distance) * 1.5
    else:
        # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –≤—ã–∑–æ–≤ —É–±–µ–∂–¥–µ–Ω–∏—è–º
        score += philosophy_distance * 1.5
        if philosophy_distance > 0.6:
            details.append("–ë—Ä–æ—Å–∞–µ—Ç –≤—ã–∑–æ–≤ –≤–∞—à–∏–º —É–±–µ–∂–¥–µ–Ω–∏—è–º")
    
    # 5. –°–¢–ò–õ–ò–°–¢–ò–ß–ï–°–ö–û–ï –°–û–û–¢–í–ï–¢–°–¢–í–ò–ï
    if material.type == 'film':
        visual_match = match_visual_preferences(material, user)
        score += visual_match * 1.0
    elif material.type == 'book':
        prose_match = match_prose_preferences(material, user)
        score += prose_match * 1.0
    
    # 6. EMOTIONAL RESONANCE
    emotional_match = calculate_emotional_resonance(
        material.emotional_parameters,
        user.emotional_profile
    )
    score += emotional_match * 0.5
    
    return {
        'score': min(10, score),
        'details': details
    }


def extract_user_theme_preferences(user):
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏
    """
    theme_ratings = defaultdict(lambda: {'sum': 0, 'count': 0})
    
    for rating in user.interaction_history:
        for theme in rating.material.themes:
            weight = theme.prominence
            theme_ratings[theme.id]['sum'] += rating.score * weight
            theme_ratings[theme.id]['count'] += weight
    
    return {
        theme_id: {
            'interest_level': data['sum'] / data['count'],
            'exposure': data['count']
        }
        for theme_id, data in theme_ratings.items()
        if data['count'] > 0
    }


def calculate_worldview_distance(material_worldview, user_worldview):
    """
    –ï–≤–∫–ª–∏–¥–æ–≤–æ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –º–µ–∂–¥—É –º–∏—Ä–æ–≤–æ–∑–∑—Ä–µ–Ω–∏—è–º–∏
    """
    dimensions = [
        'optimism_pessimism',
        'determinism_free_will',
        'individualism_collectivism',
        'idealism_pragmatism',
        'tradition_progress'
    ]
    
    distance_squared = sum(
        (material_worldview[dim] - user_worldview[dim]) ** 2
        for dim in dimensions
    )
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1
    max_distance = len(dimensions) * 4  # max distance = 2 per dimension
    return math.sqrt(distance_squared) / math.sqrt(max_distance)
```

---

### Component 3: Structural-Based Scoring (20%)

```python
def structural_based_score(material, user, weight=0.20):
    """
    –°–ª–æ–∂–Ω–æ—Å—Ç—å, —Ç–µ–º–ø, —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    """
    score = 0
    details = []
    
    # 1. –ó–û–ù–ê –ë–õ–ò–ñ–ê–ô–®–ï–ì–û –†–ê–ó–í–ò–¢–ò–Ø (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
    difficulty_diff = material.difficulty_level - user.current_complexity_comfort
    zpd_score = 0
    
    if 0 <= difficulty_diff <= 2:
        # –ò–î–ï–ê–õ–¨–ù–ê–Ø –ó–û–ù–ê –†–û–°–¢–ê
        zpd_score = 5.0
        
        if 0.5 <= difficulty_diff <= 1.5:
            zpd_score = 6.0  # –æ–ø—Ç–∏–º—É–º
            details.append("‚≠ê‚≠ê –ò–¥–µ–∞–ª—å–Ω–∞—è –∑–æ–Ω–∞ —Ä–æ—Å—Ç–∞!")
        elif difficulty_diff >= 1:
            details.append("‚≠ê –ó–æ–Ω–∞ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è")
        else:
            details.append("–ö–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å")
    
    elif -1 <= difficulty_diff < 0:
        # –ß—É—Ç—å –ø—Ä–æ—â–µ
        zpd_score = 3.5
        details.append("–ù–µ–º–Ω–æ–≥–æ –ø—Ä–æ—â–µ —Ç–µ–∫—É—â–µ–≥–æ —É—Ä–æ–≤–Ω—è")
    
    elif 2 < difficulty_diff <= 3:
        # –í—ã–∑–æ–≤
        zpd_score = 2.5
        if user.risk_tolerance >= 7:
            zpd_score = 3.5
            details.append("–ò–Ω—Ç–µ—Ä–µ—Å–Ω—ã–π –≤—ã–∑–æ–≤")
        else:
            details.append("‚ö†Ô∏è –ú–æ–∂–µ—Ç –±—ã—Ç—å —Å–ª–æ–∂–Ω–æ–≤–∞—Ç–æ")
    
    elif difficulty_diff > user.current_max_difficulty - user.current_complexity_comfort:
        # –°–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ
        zpd_score = 0.5
        details.append("‚ö†Ô∏è –í–µ—Ä–æ—è—Ç–Ω–æ —Å–ª–∏—à–∫–æ–º —Å–ª–æ–∂–Ω–æ")
    
    else:
        # –°–ª–∏—à–∫–æ–º –ø—Ä–æ—Å—Ç–æ
        zpd_score = 1.5
        if user.comfort_zone_preference > 7:
            zpd_score = 3.0  # –ª—é–±–∏—Ç –∫–æ–º—Ñ–æ—Ä—Ç
        details.append("–ú–æ–∂–µ—Ç –ø–æ–∫–∞–∑–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–æ–≤–∞—Ç—ã–º")
    
    score += zpd_score
    
    # 2. –ò–°–¢–û–ß–ù–ò–ö–ò –°–õ–û–ñ–ù–û–°–¢–ò (–¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑)
    difficulty_match = analyze_difficulty_match(
        material.difficulty_factors.difficulty_sources,
        user.cognitive_profile
    )
    score += difficulty_match * 2.0
    
    # 3. –¢–ï–ú–ü–û–†–ò–¢–ú
    pace_match = match_pacing_preference(
        material.pacing_structure.tempo,
        user.pacing_preference
    )
    score += pace_match * 1.0
    
    # 4. –°–¢–†–£–ö–¢–£–†–ù–ê–Ø –°–õ–û–ñ–ù–û–°–¢–¨
    structure_complexity = calculate_structure_complexity(material)
    user_structure_tolerance = user.cognitive_profile.pattern_recognition
    
    if structure_complexity <= user_structure_tolerance + 2:
        score += 1.0
        if structure_complexity == user_structure_tolerance + 1:
            details.append("–ù–µ–º–Ω–æ–≥–æ —Å–ª–æ–∂–Ω–µ–µ –ø—Ä–∏–≤—ã—á–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã")
    else:
        score -= 0.5
        details.append("–ù–µ—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
    
    # 5. –ö–û–ì–ù–ò–¢–ò–í–ù–ê–Ø –ù–ê–ì–†–£–ó–ö–ê
    cognitive_load = calculate_total_cognitive_load(material)
    user_capacity = user.cognitive_profile.current_max_difficulty
    
    load_ratio = cognitive_load / user_capacity
    
    if 0.6 <= load_ratio <= 1.0:
        score += 1.5  # –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    elif load_ratio > 1.2:
        score -= 1.0  # –ø–µ—Ä–µ–≥—Ä—É–∑
        details.append("–í—ã—Å–æ–∫–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞")
    
    # 6. –ü–†–û–î–û–õ–ñ–ò–¢–ï–õ–¨–ù–û–°–¢–¨ vs –î–û–°–¢–£–ü–ù–û–ï –í–†–ï–ú–Ø
    time_match = match_time_investment(
        material.time_investment,
        user.time_resources
    )
    score += time_match * 0.5
    
    return {
        'score': min(10, max(0, score)),
        'details': details,
        'zpd_flag': 0 <= difficulty_diff <= 2
    }


def analyze_difficulty_match(material_sources, user_profile):
    """
    –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    """
    match_score = 0
    
    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–º–∏ —Å—Ç–∏–ª—è–º–∏
    # –ø–æ-—Ä–∞–∑–Ω–æ–º—É —Å–ø—Ä–∞–≤–ª—è—é—Ç—Å—è —Å —Ä–∞–∑–Ω—ã–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏
    
    style_strengths = {
        'visual': {'structural': 0.8, 'linguistic': 0.5, 'conceptual': 0.7},
        'analytical': {'structural': 1.0, 'linguistic': 0.7, 'conceptual': 1.0},
        'empathetic': {'structural': 0.6, 'linguistic': 0.8, 'conceptual': 0.7},
        'kinesthetic': {'structural': 0.7, 'linguistic': 0.6, 'conceptual': 0.6}
    }
    
    user_strengths = style_strengths[user_profile.primary_cognitive_style]
    
    for source, difficulty in material_sources.items():
        strength = user_strengths.get(source, 0.7)
        
        # –ú–æ–∂–µ—Ç —Å–ø—Ä–∞–≤–∏—Ç—å—Å—è?
        if difficulty * strength <= user_profile.current_max_difficulty:
            match_score += 1.0
        elif difficulty * strength <= user_profile.current_max_difficulty + 1:
            match_score += 0.5  # –Ω–∞ –≥—Ä–∞–Ω–∏
        else:
            match_score -= 0.5  # –ø–µ—Ä–µ–≥—Ä—É–∑
    
    return match_score / len(material_sources)


def calculate_total_cognitive_load(material):
    """
    –°—É–º–º–∞—Ä–Ω–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –Ω–∞–≥—Ä—É–∑–∫–∞
    """
    load = 0
    
    # –°—Ç—Ä—É–∫—Ç—É—Ä–Ω–∞—è —Å–ª–æ–∂–Ω–æ—Å—Ç—å
    load += material.pacing_structure.narrative_structure.timeline_complexity * 0.5
    load += (1 - material.pacing_structure.narrative_structure.linearity) * 2
    
    # –¢—Ä–æ–ø—ã
    for trope in material.analyzed_tropes:
        load += trope.cognitive_load * trope.centrality * 0.1
    
    # –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø–ª–æ—Ç–Ω–æ—Å—Ç—å
    load += material.thematic_parameters.thematic_density * 0.3
    
    # –§–∏–ª–æ—Å–æ—Ñ—Å–∫–∞—è –≥–ª—É–±–∏–Ω–∞
    load += material.thematic_parameters.philosophical_depth.overall * 0.2
    
    # –ú–µ—Ç–∞–Ω–∞—Ä—Ä–∞—Ç–∏–≤
    if material.trope_engagement:
        load += material.trope_engagement.meta_awareness * 0.3
    
    return load
```

---

### Component 4: Transformation-Based Scoring (15%)

```python
def transformation_based_score(material, user, weight=0.15):
    """
    –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª —Å —É—á—ë—Ç–æ–º –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    """
    base_score = material.transformative_score
    
    # 1. –ö–û–†–†–ï–ö–¢–ò–†–û–í–ö–ê –ù–ê –†–ê–ë–û–¢–£ –° –¢–†–û–ü–ê–ú–ò
    if material.trope_engagement:
        if material.trope_engagement.transformation_depth >= 7:
            base_score += 0.5
        if material.trope_engagement.meta_awareness >= 8:
            base_score += 0.5
    
    # 2. –ì–û–¢–û–í–ù–û–°–¢–¨ –ö –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ò
    readiness_multiplier = calculate_transformation_readiness(
        material.transformative_potential,
        user
    )
    
    adjusted_score = base_score * readiness_multiplier
    
    # 3. –°–ü–ï–¶–ò–§–ò–ß–ï–°–ö–ò–ï –ú–ï–•–ê–ù–ò–ó–ú–´
    mechanism_match = 0
    
    for mechanism in material.transformative_potential.mechanisms:
        if mechanism in user.goals_motivation.target_skills:
            mechanism_match += 0.5
    
    adjusted_score += min(1.0, mechanism_match)
    
    # 4. –¶–ï–õ–ï–í–´–ï –ò–ó–ú–ï–ù–ï–ù–ò–Ø
    if material.transformative_potential.target_changes:
        alignment = calculate_goal_alignment(
            material.transformative_potential.target_changes,
            user.goals_motivation
        )
        adjusted_score += alignment * 1.0
    
    # 5. –ë–û–ù–£–° –ó–ê –†–ïWATCH VALUE
    if material.rewatch_value >= 8:
        adjusted_score += 0.5
    
    return {
        'score': min(10, adjusted_score),
        'readiness': readiness_multiplier
    }


def calculate_transformation_readiness(potential, user):
    """
    –ù–∞—Å–∫–æ–ª—å–∫–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ—Ç–æ–≤ –∫ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏
    """
    # –ë–∞–∑–æ–≤–∞—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å
    readiness = 1.0
    
    # –¢–æ–ª–µ—Ä–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫ –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–º—É –¥–∏—Å—Å–æ–Ω–∞–Ω—Å—É
    if potential.components.cognitive_dissonance > 0.7:
        if user.cognitive_profile.tolerance_for_ambiguity < 5:
            readiness *= 0.7  # –Ω–µ –æ—á–µ–Ω—å –≥–æ—Ç–æ–≤
        else:
            readiness *= 1.2  # –≥–æ—Ç–æ–≤ –∫ –≤—ã–∑–æ–≤—É
    
    # –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ —Å–º–µ–Ω–µ —É–±–µ–∂–¥–µ–Ω–∏–π
    if potential.components.perspective_shifting > 0.7:
        if user.thematic_preferences.challenge_appetite.belief_challenging < 5:
            readiness *= 0.8
        else:
            readiness *= 1.3
    
    # –ú–µ—Ç–∞–∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–∞—è –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å
    if potential.components.metacognitive_awareness > 0.7:
        meta_diff = user.meta_awareness_level - 7
        if meta_diff < 0:
            readiness *= (1 + meta_diff * 0.1)  # —à—Ç—Ä–∞—Ñ –µ—Å–ª–∏ –Ω–µ –≥–æ—Ç–æ–≤
        else:
            readiness *= 1.1  # –±–æ–Ω—É—Å –µ—Å–ª–∏ –≥–æ—Ç–æ–≤
    
    # –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å
    entry_barrier = potential.transformation_journey.entry_barrier
    if entry_barrier > user.emotional_profile.max_emotional_intensity:
        readiness *= 0.6  # —Å–ª–∏—à–∫–æ–º –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ
    
    return max(0.5, min(1.5, readiness))
```

---

### Component 5: Collaborative Filtering (10%)

```python
def collaborative_filtering_score(material, user, weight=0.10):
    """
    –ß—Ç–æ –æ—Ü–µ–Ω–∏–ª–∏ –ø–æ—Ö–æ–∂–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
    """
    # –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    similar_users = find_similar_users(user, k=50)
    
    if not similar_users:
        return {'score': 5.0, 'confidence': 0.0}  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π
    
    # –°–æ–±—Ä–∞—Ç—å –∏—Ö –æ—Ü–µ–Ω–∫–∏ —ç—Ç–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    ratings = []
    similarities = []
    
    for similar_user, similarity in similar_users:
        rating = get_user_rating(similar_user.id, material.id)
        if rating:
            ratings.append(rating)
            similarities.append(similarity)
    
    if not ratings:
        return {'score': 5.0, 'confidence': 0.0}
    
    # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
    weighted_avg = np.average(ratings, weights=similarities)
    
    # Confidence –∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—Ü–µ–Ω–æ–∫
    confidence = min(1.0, len(ratings) / 10)
    
    return {
        'score': weighted_avg,
        'confidence': confidence
    }


def find_similar_users(user, k=50):
    """
    User-based collaborative filtering
    """
    # 1. –°–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_vector = create_user_preference_vector(user)
    
    # 2. –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏—Ö —á–µ—Ä–µ–∑ –∫–æ—Å–∏–Ω—É—Å–Ω—É—é –±–ª–∏–∑–æ—Å—Ç—å
    all_users = get_all_users_with_sufficient_ratings(min_ratings=10)
    
    similarities = []
    for other_user in all_users:
        if other_user.id == user.id:
            continue
        
        other_vector = create_user_preference_vector(other_user)
        similarity = cosine_similarity(user_vector, other_vector)
        
        similarities.append((other_user, similarity))
    
    # –¢–æ–ø-k —Å–∞–º—ã—Ö –ø–æ—Ö–æ–∂–∏—Ö
    similarities.sort(key=lambda x: x[1], reverse=True)
    return similarities[:k]


def create_user_preference_vector(user):
    """
    –°–æ–∑–¥–∞—ë—Ç –≤–µ–∫—Ç–æ—Ä –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏–π
    Dimensions: [genre_preferences, theme_preferences, creator_preferences, 
                 complexity_level, cognitive_style_encoding, ...]
    """
    vector = []
    
    # –ñ–∞–Ω—Ä—ã (normalized)
    for genre in STANDARD_GENRES:
        vector.append(user.genre_preferences.get(genre, 5) / 10)
    
    # –¢–µ–º—ã (top 50)
    for theme in STANDARD_THEMES:
        vector.append(user.theme_preferences.get(theme, 5) / 10)
    
    # –ö–æ–≥–Ω–∏—Ç–∏–≤–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å
    vector.append(user.cognitive_profile.current_complexity_comfort / 10)
    vector.append(user.meta_awareness_level / 10)
    
    # One-hot encoding –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–≥–æ —Å—Ç–∏–ª—è
    style_encoding = [0, 0, 0, 0]
    style_index = ['visual', 'analytical', 'empathetic', 'kinesthetic'].index(
        user.primary_cognitive_style
    )
    style_encoding[style_index] = 1
    vector.extend(style_encoding)
    
    return np.array(vector)
```

---

### Component 6: Popularity-Based Scoring (5%)

```python
def popularity_based_score(material, weight=0.05):
    """
    –ü–æ–ø—É–ª—è—Ä–Ω–æ—Å—Ç—å –∏ –∫—É–ª—å—Ç—É—Ä–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
    """
    score = 0
    
    # 1. –†–µ–π—Ç–∏–Ω–≥–∏
    if material.critical_acclaim:
        score += (material.critical_acclaim / 10) * 2.0
    
    # 2. –ö—É–ª—å—Ç—É—Ä–Ω—ã–π –∏–º–ø–∞–∫—Ç
    if material.cultural_context.cultural_impact:
        impact = material.cultural_context.cultural_impact
        score += (impact.mainstream_penetration / 10) * 2.0
        score += (impact.longevity / 10) * 1.5
    
    # 3. –ö–∞–Ω–æ–Ω–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å
    if material.cultural_context.canonical_status:
        score += material.cultural_context.canonical_status * 2.5
    
    # 4. Frequency of recommendation
    recommendation_frequency = get_recommendation_frequency(material.id)
    score += min(2.0, recommendation_frequency / 100 * 2.0)
    
    return {'score': min(10, score)}
```

---

### Component 7: Context-Aware Scoring (5%)

```python
def context_aware_score(material, user, session_context, weight=0.05):
    """
    –£—á—ë—Ç —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è
    """
    score = 5.0  # neutral base
    
    # 1. –í–†–ï–ú–Ø –°–£–¢–û–ö
    if session_context.time_of_day == 'late_night':
        # –ù–æ—á—å—é –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ –º–µ–Ω–µ–µ –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–µ
        if material.emotional_intensity < 6:
            score += 1.5
        elif material.emotional_intensity > 8:
            score -= 1.0
    
    # 2. ENERGY LEVEL
    if session_context.user_state.energy_level < 5:
        # –ù–∏–∑–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è - –ø—Ä–æ—â–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
        adjusted_difficulty = material.difficulty_level - 1.5
        if adjusted_difficulty <= user.current_complexity_comfort:
            score += 1.5
    
    # 3. –ù–ê–°–¢–†–û–ï–ù–ò–ï
    mood_match = match_mood_to_material(
        session_context.user_state.emotional_state,
        material.emotional_parameters
    )
    score += mood_match * 1.0
    
    # 4. –°–û–¶–ò–ê–õ–¨–ù–´–ô –ö–û–ù–¢–ï–ö–°–¢
    if session_context.alone_or_with_others == 'with_others':
        # –ì—Ä—É–ø–ø–∞ - –ª—É—á—à–µ –º–µ–Ω–µ–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ
        if material.accessibility.suitable_for_background:
            score += 0.5
        if 'experimental' in material.genres:
            score -= 1.0
    
    # 5. IMMEDIATE NEEDS
    for need, level in session_context.immediate_needs.items():
        material_satisfaction = estimate_need_satisfaction(material, need)
        score += (material_satisfaction * level / 10) * 0.5
    
    # 6. SEASONAL/CULTURAL TIMING
    if session_context.external_factors:
        seasonal_bonus = calculate_seasonal_relevance(
            material,
            session_context.external_factors
        )
        score += seasonal_bonus
    
    return {'score': min(10, max(0, score))}


def match_mood_to_material(user_mood, material_emotions):
    """
    –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª—É
    """
    # –ò–Ω–æ–≥–¥–∞ —Ö–æ—Ç–∏–º matching mood, –∏–Ω–æ–≥–¥–∞ - complementary
    # –£–ø—Ä–æ—â—ë–Ω–Ω–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞
    
    if user_mood in ['sad', 'melancholic']:
        # –ï—Å–ª–∏ –≥—Ä—É—Å—Ç–Ω–æ - catharsis –∏–ª–∏ uplift?
        if material_emotions.catharsis_potential > 7:
            return 1.5  # –∫–∞—Ç–∞—Ä—Å–∏—Å
        if material_emotions.final_tone > 0.5:
            return 1.0  # –ø–æ–¥–Ω—è—Ç–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è
    
    elif user_mood in ['stressed', 'anxious']:
        # –ï—Å–ª–∏ —Å—Ç—Ä–µ—Å—Å - —Ä–∞—Å—Å–ª–∞–±–ª–µ–Ω–∏–µ
        if material_emotions.emotional_intensity < 5:
            return 1.5
    
    elif user_mood in ['energetic', 'excited']:
        # –ï—Å–ª–∏ —ç–Ω–µ—Ä–≥–∏—è - –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–µ
        if material_emotions.emotional_intensity > 7:
            return 1.5
    
    return 0.0  # –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–π match
```

---

### Component 8: Serendipity Injection (5%)

```python
def serendipity_score(material, user, randomness_factor=0.05):
    """
    –≠–ª–µ–º–µ–Ω—Ç –Ω–µ–æ–∂–∏–¥–∞–Ω–Ω–æ—Å—Ç–∏ –∏ –æ—Ç–∫—Ä—ã—Ç–∏–π
    """
    # –°–ª—É—á–∞–π–Ω—ã–π –±–æ–Ω—É—Å –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
    random_boost = np.random.normal(0, 2.0) * randomness_factor
    
    # –ë–æ–Ω—É—Å –∑–∞ –Ω–æ–≤–∏–∑–Ω—É
    novelty_bonus = 0
    
    # 1. –ù–µ–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–Ω—ã–µ –∂–∞–Ω—Ä—ã
    if material.primary_genre not in user.genre_literacy:
        novelty_bonus += 1.5
    
    # 2. –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Ç—Ä–æ–ø—ã
    unknown_tropes = [
        t for t in material.analyzed_tropes
        if f"{t.trope_id}_{t.usage_type}" not in user.trope_preferences
    ]
    if len(unknown_tropes) >= 3:
        novelty_bonus += 1.0
    
    # 3. –ù–µ–æ–±—ã—á–Ω—ã–π —Å–æ–∑–¥–∞—Ç–µ–ª—å
    if material.creator not in user.creator_preferences:
        novelty_bonus += 0.5
    
    # 4. –ó–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ comfort zone
    if material.difficulty_level > user.current_max_difficulty:
        if user.exploration_strategy.risk_tolerance > 6:
            novelty_bonus += 1.5  # –ª—é–±–∏—Ç —Ä–∏—Å–∫
    
    # 5. –í—ã—Å–æ–∫–∞—è –∫—É–ª—å—Ç—É—Ä–Ω–∞—è –∑–Ω–∞—á–∏–º–æ—Å—Ç—å –Ω–æ –Ω–µ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–æ
    if (material.cultural_context.canonical_status > 0.7 and
        material.id not in user.viewed_materials):
        novelty_bonus += 1.0
    
    score = 5.0 + random_boost + novelty_bonus
    
    return {'score': min(10, max(0, score))}
```

---

### Ensemble & Calibration

```python
def ensemble_recommendations(material, user, session_context):
    """
    –ö–æ–º–±–∏–Ω–∏—Ä—É–µ—Ç –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –≤ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π score
    """
    # 1. COMPUTE ALL COMPONENT SCORES
    components = {
        'trope_based': trope_based_score(material, user, weight=0.25),
        'content_based': content_based_score(material, user, weight=0.20),
        'structural_based': structural_based_score(material, user, weight=0.20),
        'transformation_based': transformation_based_score(material, user, weight=0.15),
        'collaborative': collaborative_filtering_score(material, user, weight=0.10),
        'popularity': popularity_based_score(material, weight=0.05),
        'context_aware': context_aware_score(material, user, session_context, weight=0.05),
        'serendipity': serendipity_score(material, user, randomness_factor=0.05)
    }
    
    # 2. ADAPTIVE WEIGHTING
    # –í–µ—Å–∞ –º–æ–≥—É—Ç –º–µ–Ω—è—Ç—å—Å—è –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    weights = calculate_adaptive_weights(user, session_context)
    
    # 3. WEIGHTED ENSEMBLE
    weighted_score = sum(
        components[name]['score'] * weights[name]
        for name in components.keys()
    )
    
    # 4. CALIBRATION
    # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    calibrated_score = calibrate_score(
        weighted_score,
        material,
        user
    )
    
    # 5. CONFIDENCE ESTIMATION
    confidence = estimate_confidence(components, user)
    
    return {
        'final_score': calibrated_score,
        'components': components,
        'weights': weights,
        'confidence': confidence
    }


def calculate_adaptive_weights(user, session_context):
    """
    –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –≤–µ—Å–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
    """
    base_weights = {
        'trope_based': 0.25,
        'content_based': 0.20,
        'structural_based': 0.20,
        'transformation_based': 0.15,
        'collaborative': 0.10,
        'popularity': 0.05,
        'context_aware': 0.05,
        'serendipity': 0.00  # –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ
    }
    
    # –ê–î–ê–ü–¢–ê–¶–ò–Ø
    
    # –ï—Å–ª–∏ –Ω–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å - –±–æ–ª—å—à–µ popularity
    if len(user.interaction_history) < 10:
        base_weights['popularity'] += 0.10
        base_weights['collaborative'] -= 0.05
        base_weights['trope_based'] -= 0.05
    
    # –ï—Å–ª–∏ –≤—ã—Å–æ–∫–∞—è —ç–Ω–µ—Ä–≥–∏—è - –±–æ–ª—å—à–µ structure/transformation
    if session_context.user_state.energy_level > 7:
        base_weights['structural_based'] += 0.05
        base_weights['transformation_based'] += 0.05
        base_weights['context_aware'] -= 0.10
    
    # –ï—Å–ª–∏ –∏—â–µ—Ç –æ—Ç–∫—Ä—ã—Ç–∏—è - –±–æ–ª—å—à–µ serendipity
    if session_context.immediate_needs.get('inspiration', 0) > 7:
        base_weights['serendipity'] = 0.10
        base_weights['content_based'] -= 0.05
        base_weights['collaborative'] -= 0.05
    
    # –ï—Å–ª–∏ —á—ë—Ç–∫–∏–µ —Ü–µ–ª–∏ –æ–±—É—á–µ–Ω–∏—è - –±–æ–ª—å—à–µ transformation
    if user.goals_motivation.primary_goals and 'intellectual_growth' in user.goals_motivation.primary_goals:
        base_weights['transformation_based'] += 0.10
        base_weights['popularity'] -= 0.05
        base_weights['context_aware'] -= 0.05
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    total = sum(base_weights.values())
    return {k: v/total for k, v in base_weights.items()}


def calibrate_score(raw_score, material, user):
    """
    –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π —Ç–æ—á–Ω–æ—Å—Ç–∏
    """
    # –ü–æ–ª—É—á–∏—Ç—å –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —ç—Ç–æ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞
    past_predictions = get_past_predictions(material.id)
    
    if past_predictions:
        # –°—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        avg_error = np.mean([
            p.predicted_score - p.actual_rating
            for p in past_predictions
            if p.actual_rating is not None
        ])
        
        # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º
        calibrated = raw_score - avg_error
    else:
        calibrated = raw_score
    
    # –¢–∞–∫–∂–µ —É—á–∏—Ç—ã–≤–∞–µ–º bias –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_bias = calculate_user_rating_bias(user)
    calibrated += user_bias
    
    return max(0, min(10, calibrated))


def calculate_user_rating_bias(user):
    """
    –°–∫–ª–æ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∑–∞–≤—ã—à–∞—Ç—å/–∑–∞–Ω–∏–∂–∞—Ç—å –æ—Ü–µ–Ω–∫–∏
    """
    if not user.interaction_history:
        return 0
    
    user_avg = np.mean([r.rating for r in user.interaction_history])
    global_avg = 6.5  # –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º–æ–µ —Å—Ä–µ–¥–Ω–µ–µ
    
    return (user_avg - global_avg) * 0.3  # —á–∞—Å—Ç–∏—á–Ω–∞—è –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞
```

---

### Post-Processing & Filtering

```python
def post_process_recommendations(
    scored_materials,
    user,
    session_context,
    top_k=20
):
    """
    –§–∏–Ω–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è, diversification, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
    """
    
    # 1. SAFETY FILTERS (–ö–†–ò–¢–ò–ß–ù–û!)
    safe_materials = apply_safety_filters(scored_materials, user)
    
    # 2. DIVERSITY OPTIMIZATION
    diverse_materials = optimize_diversity(safe_materials, user, k=top_k*3)
    
    # 3. ZPD PRIORITIZATION
    zpd_boosted = boost_zpd_materials(diverse_materials, user)
    
    # 4. RECENCY FILTER
    # –ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –Ω–µ–¥–∞–≤–Ω–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω–æ–µ
    fresh_materials = filter_recent(zpd_boosted, user, days=90)
    
    # 5. SORTING & SELECTION
    final_recommendations = fresh_materials[:top_k]
    
    # 6. EXPLANATION GENERATION
    for rec in final_recommendations:
        rec['explanation'] = generate_explanation(
            rec,
            user,
            session_context
        )
    
    # 7. CONFIDENCE BADGES
    for rec in final_recommendations:
        rec['badges'] = generate_badges(rec, user)
    
    return final_recommendations


def apply_safety_filters(materials, user):
    """
    –ö–†–ò–¢–ò–ß–ù–û: –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤ –∏ –Ω–µ–ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
    """
    safe = []
    
    for material in materials:
        # 1. –¢—Ä–∏–≥–≥–µ—Ä—ã (100% —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
        if any(trigger in user.avoid_triggers 
               for trigger in material.trigger_warnings):
            continue
        
        # 2. –≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ—Å—Ç—å
        if material.emotional_intensity > user.emotional_profile.max_emotional_intensity:
            continue
        
        # 3. –°–ª–æ–∂–Ω–æ—Å—Ç—å (–Ω–µ –±–æ–ª–µ–µ +4 –æ—Ç comfort)
        if material.difficulty_level > user.current_max_difficulty + 1:
            continue
        
        # 4. –í–æ–∑—Ä–∞—Å—Ç–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è
        if not check_age_appropriate(material, user):
            continue
        
        safe.append(material)
    
    return safe


def optimize_diversity(materials, user, k=60):
    """
    –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
    –ò—Å–ø–æ–ª—å–∑—É–µ–º Maximum Marginal Relevance (MMR)
    """
    if len(materials) <= k:
        return materials
    
    selected = []
    remaining = materials.copy()
    
    # –ü–µ—Ä–≤—ã–π - —Å–∞–º—ã–π —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π
    selected.append(remaining.pop(0))
    
    lambda_param = 0.7  # balance relevance vs diversity
    
    while len(selected) < k and remaining:
        best_score = -float('inf')
        best_idx = 0
        
        for idx, candidate in enumerate(remaining):
            # Relevance
            relevance = candidate['final_score']
            
            # Diversity (–º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ —Å —É–∂–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–º–∏)
            min_similarity = min(
                calculate_material_similarity(candidate, selected_item)
                for selected_item in selected
            )
            diversity = 1 - min_similarity
            
            # MMR score
            mmr_score = lambda_param * relevance + (1 - lambda_param) * diversity
            
            if mmr_score > best_score:
                best_score = mmr_score
                best_idx = idx
        
        selected.append(remaining.pop(best_idx))
    
    return selected


def calculate_material_similarity(mat1, mat2):
    """
    –ö–æ—Å–∏–Ω—É—Å–Ω–∞—è –±–ª–∏–∑–æ—Å—Ç—å –º–µ–∂–¥—É –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º–∏
    """
    # –°–æ–∑–¥–∞—Ç—å –≤–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    vec1 = create_material_feature_vector(mat1)
    vec2 = create_material_feature_vector(mat2)
    
    return cosine_similarity(vec1, vec2)


def boost_zpd_materials(materials, user):
    """
    –î–∞—ë–º –±–æ–Ω—É—Å –º–∞—Ç–µ—Ä–∏–∞–ª–∞–º –≤ –∑–æ–Ω–µ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è
    """
    for material in materials:
        if material.get('components', {}).get('structural_based', {}).get('zpd_flag'):
            material['final_score'] *= 1.15  # 15% –±–æ–Ω—É—Å
            material['zpd_boosted'] = True
    
    # –ü–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
    materials.sort(key=lambda m: m['final_score'], reverse=True)
    return materials


def generate_explanation(recommendation, user, session_context):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —á–µ–ª–æ–≤–µ–∫–æ—á–∏—Ç–∞–µ–º–æ–≥–æ –æ–±—ä—è—Å–Ω–µ–Ω–∏—è
    """
    explanation_parts = []
    components = recommendation['components']
    
    # –°–∞–º—ã–π —Å–∏–ª—å–Ω—ã–π –∫–æ–º–ø–æ–Ω–µ–Ω—Ç
    strongest = max(components.items(), key=lambda x: x[1]['score'])
    
    if strongest[0] == 'trope_based':
        details = strongest[1].get('details', [])
        if details:
            explanation_parts.append(details[0])
    
    elif strongest[0] == 'content_based':
        details = strongest[1].get('details', [])
        if details:
            explanation_parts.append(details[0])
    
    elif strongest[0] == 'structural_based':
        if recommendation.get('zpd_boosted'):
            explanation_parts.append("–ò–¥–µ–∞–ª—å–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –≤–∞—à–µ–≥–æ —Ä–æ—Å—Ç–∞")
    
    # ZPD flag (–æ—á–µ–Ω—å –≤–∞–∂–µ–Ω)
    if recommendation.get('zpd_boosted'):
        explanation_parts.insert(0, "‚≠ê –í –∑–æ–Ω–µ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—è")
    
    # Transformation
    if components['transformation_based']['score'] >= 8:
        explanation_parts.append("–í—ã—Å–æ–∫–∏–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª")
    
    # Context relevance
    if components['context_aware']['score'] >= 7:
        explanation_parts.append("–ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è")
    
    return " ‚Ä¢ ".join(explanation_parts[:3])  # —Ç–æ–ø-3 –ø—Ä–∏—á–∏–Ω—ã


def generate_badges(recommendation, user):
    """
    –í–∏–∑—É–∞–ª—å–Ω—ã–µ –±–µ–π–¥–∂–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
    """
    badges = []
    
    # ZPD
    if recommendation.get('zpd_boosted'):
        badges.append({'type': 'zpd', 'label': '–ó–æ–Ω–∞ —Ä–æ—Å—Ç–∞', 'color': 'gold'})
    
    # High transformation
    if recommendation['components']['transformation_based']['score'] >= 8.5:
        badges.append({'type': 'transform', 'label': '–¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è', 'color': 'purple'})
    
    # Perfect match
    if recommendation['final_score'] >= 9.0:
        badges.append({'type': 'match', 'label': '–û—Ç–ª–∏—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ', 'color': 'green'})
    
    # Canonical
    material = recommendation['material']
    if material.cultural_context and material.cultural_context.canonical_status > 0.8:
        badges.append({'type': 'canonical', 'label': '–ö–ª–∞—Å—Å–∏–∫–∞', 'color': 'blue'})
    
    # Challenge
    difficulty_diff = material.difficulty_level - user.current_complexity_comfort
    if difficulty_diff > 2:
        badges.append({'type': 'challenge', 'label': '–í—ã–∑–æ–≤', 'color': 'orange'})
    
    return badges
```

---

## üîÑ CONTINUOUS LEARNING

```python
class RecommendationSystem:
    """
    –°–∏—Å—Ç–µ–º–∞ —Å –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º
    """
    
    def __init__(self):
        self.model = load_pretrained_model()
        self.feedback_buffer = []
    
    def get_recommendations(self, user_id, session_context, top_k=20):
        """
        –ì–ª–∞–≤–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        """
        user = load_user_profile(user_id)
        all_materials = load_candidate_materials(user)
        
        scored_materials = []
        
        for material in all_materials:
            # Ensemble scoring
            result = ensemble_recommendations(
                material,
                user,
                session_context
            )
            
            scored_materials.append({
                'material': material,
                **result
            })
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞
        scored_materials.sort(
            key=lambda m: m['final_score'],
            reverse=True
        )
        
        # Post-processing
        recommendations = post_process_recommendations(
            scored_materials,
            user,
            session_context,
            top_k=top_k
        )
        
        # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.log_recommendations(user_id, recommendations)
        
        return recommendations
    
    def record_feedback(self, user_id, material_id, interaction_data):
        """
        –ó–∞–ø–∏—Å–∞—Ç—å –æ–±—Ä–∞—Ç–Ω—É—é —Å–≤—è–∑—å
        """
        feedback = {
            'user_id': user_id,
            'material_id': material_id,
            'timestamp': datetime.now(),
            **interaction_data
        }
        
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ –ë–î
        save_interaction(feedback)
        
        # –î–æ–±–∞–≤–∏—Ç—å –≤ –±—É—Ñ–µ—Ä –¥–ª—è online learning
        self.feedback_buffer.append(feedback)
        
        # –ï—Å–ª–∏ –±—É—Ñ–µ—Ä –∑–∞–ø–æ–ª–Ω–µ–Ω - –æ–±–Ω–æ–≤–∏—Ç—å –º–æ–¥–µ–ª—å
        if len(self.feedback_buffer) >= 100:
            self.update_model()
    
    def update_model(self):
        """
        –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        """
        # –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
        training_data = prepare_training_data(self.feedback_buffer)
        
        # Fine-tune embeddings
        self.model.fine_tune(training_data)
        
        # –û–±–Ω–æ–≤–∏—Ç—å –∫—ç—à–∏
        invalidate_caches()
        
        # –û—á–∏—Å—Ç–∏—Ç—å –±—É—Ñ–µ—Ä
        self.feedback_buffer = []
    
    def analyze_prediction_quality(self):
        """
        –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
        """
        recent_predictions = load_recent_predictions(days=30)
        
        metrics = {
            'rmse': calculate_rmse(recent_predictions),
            'mae': calculate_mae(recent_predictions),
            'precision_at_10': calculate_precision_at_k(recent_predictions, k=10),
            'ndcg': calculate_ndcg(recent_predictions),
            'diversity': calculate_diversity_metric(recent_predictions),
            'serendipity': calculate_serendipity_metric(recent_predictions)
        }
        
        return metrics
```

---

## üìà –ú–ï–¢–†–ò–ö–ò –û–¶–ï–ù–ö–ò

```python
def evaluate_recommendation_quality(predictions, actual_interactions):
    """
    Comprehensive evaluation
    """
    metrics = {}
    
    # 1. ACCURACY METRICS
    metrics['rmse'] = np.sqrt(np.mean([
        (p.predicted_score - a.actual_rating) ** 2
        for p, a in zip(predictions, actual_interactions)
        if a.actual_rating is not None
    ]))
    
    # 2. RANKING METRICS
    metrics['precision@10'] = calculate_precision_at_k(predictions, actual_interactions, k=10)
    metrics['recall@20'] = calculate_recall_at_k(predictions, actual_interactions, k=20)
    metrics['ndcg'] = calculate_normalized_dcg(predictions, actual_interactions)
    
    # 3. DIVERSITY METRICS
    metrics['intra_list_diversity'] = calculate_intra_list_diversity(predictions)
    metrics['genre_diversity'] = calculate_genre_diversity(predictions)
    
    # 4. COVERAGE
    metrics['catalog_coverage'] = len(set(p.material_id for p in predictions)) / total_materials
    
    # 5. NOVELTY
    metrics['novelty'] = calculate_novelty(predictions, user)
    
    # 6. SERENDIPITY
    metrics['serendipity'] = calculate_serendipity(predictions, user)
    
    # 7. TRANSFORMATION METRICS (—É–Ω–∏–∫–∞–ª—å–Ω–æ –¥–ª—è –Ω–∞—à–µ–π —Å–∏—Å—Ç–µ–º—ã)
    metrics['avg_transformation_score'] = np.mean([
        p.material.transformative_score
        for p in predictions[:20]
    ])
    
    metrics['zpd_hit_rate'] = sum(
        1 for p in predictions[:20]
        if p.get('zpd_boosted')
    ) / 20
    
    # 8. USER SATISFACTION
    metrics['user_satisfaction'] = calculate_user_satisfaction(actual_interactions)
    
    return metrics
```

---

## üéØ –ó–ê–ö–õ–Æ–ß–ï–ù–ò–ï

–≠—Ç–æ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø–æ–ª–Ω—ã–π –∞–ª–≥–æ—Ä–∏—Ç–º –≤–∫–ª—é—á–∞–µ—Ç:

‚úÖ **89+ –∫–∞—Ç–µ–≥–æ—Ä–∏–π –¥–∞–Ω–Ω—ã—Ö** –æ –º–∞—Ç–µ—Ä–∏–∞–ª–∞—Ö
‚úÖ **–ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–π –ø—Ä–æ—Ñ–∏–ª—å** –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
‚úÖ **8 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤** —Å–∫–æ—Ä–∏–Ω–≥–∞ (tropes, content, structural, transformation, collaborative, popularity, context, serendipity)
‚úÖ **Adaptive weighting** –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
‚úÖ **ML embeddings** –¥–ª—è —Ç—Ä–æ–ø–æ–≤ –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
‚úÖ **Safety filters** (100% —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ç—Ä–∏–≥–≥–µ—Ä–æ–≤)
‚úÖ **Diversity optimization** (MMR)
‚úÖ **ZPD prioritization**
‚úÖ **Continuous learning** —Å –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑—å—é
‚úÖ **Comprehensive metrics**

**–°–ª–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:** –û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è (6-12 –º–µ—Å—è—Ü–µ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏)
**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –¥–∞–Ω–Ω—ã–º:** –ú–∏–Ω–∏–º—É–º 1000+ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ —Å –ø–æ–ª–Ω–æ–π –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π, 500+ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
**–ö–∞—á–µ—Å—Ç–≤–æ:** State-of-the-art –¥–ª—è domain-specific recommendations

–î–ª—è **MVP** –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ —É–ø—Ä–æ—â—ë–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é —Å 3-4 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏, –∞ –∑–∞—Ç–µ–º –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ! üöÄ